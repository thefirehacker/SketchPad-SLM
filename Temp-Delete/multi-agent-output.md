🤖 Multi-Agent Process Details
════════════════════════════════════════

1. DataInspector ✅
   Type: DataInspector
   Status: completed
   Progress: 100%
   Stage: Completed
   Duration: 189681ms
   
   📤 Full Output:
   {
     "result": "success",
     "output": {
       "documentAnalysis": {
         "documentType": "Multi-Document Analysis",
         "structure": [
           "** Research Paper",
           "** Research Paper/Technical Report"
         ],
         "contentAreas": [
           "Based on the provided document type (\"Research Paper\") and the sample content (\"Document metadata: GRPO_Paper.pdf\")",
           "here's a breakdown of the likely structure and information types:\n\n**Main Content Areas/Sections:**\n\n*   Abstract\n*   Introduction\n*   Literature Review\n*   Methodology\n*   Results\n*   Discussion\n*   Conclusion\n*   References/Bibliography\n*   Appendices (potentially)\n\n**Specific Types of Information Present:**\n\n*   Title\n*   Author(s)\n*   Date\n*   Keywords\n*   Abstract (summary of the paper)\n*   Background information (related to the research topic)\n*   Research questions/hypotheses\n*   Materials/Equipment used\n*   Data analysis techniques\n*   Statistical results (tables",
           "figures",
           "and statistical tests)\n*   Interpretation of results\n*   Limitations of the study\n*   Future research directions\n*   Citations (references to other works)\n*   Data (potentially raw or processed data)\n*   Figures/Tables (visual representations of data)",
           "Here's an analysis of the document type and sample content",
           "outlining the likely structure and information types:\n\n**Main Content Areas/Sections:**\n\n*   Abstract/Introduction\n*   Literature Review\n*   Methodology/Approach\n*   Results/Findings\n*   Discussion\n*   Conclusion\n*   References/Bibliography\n*   Appendices (potentially)\n\n**Specific Types of Information:**\n\n*   Title (LLMS FOR ENGINEERING- TEACHIING MODELS TO DESIGN HIGH POWERED ROCKETS)\n*   Document Metadata (filename",
           "version number)\n*   Text (likely including technical terms",
           "equations",
           "and descriptions)\n*   Data (potentially numerical data",
           "graphs",
           "tables)\n*   Code (potentially code snippets related to the methodology)\n*   References (citations to other research papers)\n*   Keywords (implied by the title)"
         ],
         "queryIntent": "Extract information from 2 relevant documents",
         "extractionStrategy": "Extract from each relevant document separately with proper attribution",
         "expectedOutputFormat": "structured synthesis with proper attribution",
         "documents": [
           {
             "documentId": "doc_1754658663251_vpl5u967b",
             "documentName": "doc_1754658663251_vpl5u967b",
             "documentType": "** Research Paper",
             "primaryEntity": "** Reinforcement Learning",
             "structure": [
               "** research paper sections"
             ],
             "contentAreas": [
               "Based on the provided document type (\"Research Paper\") and the sample content (\"Document metadata: GRPO_Paper.pdf\")",
               "here's a breakdown of the likely structure and information types:\n\n**Main Content Areas/Sections:**\n\n*   Abstract\n*   Introduction\n*   Literature Review\n*   Methodology\n*   Results\n*   Discussion\n*   Conclusion\n*   References/Bibliography\n*   Appendices (potentially)\n\n**Specific Types of Information Present:**\n\n*   Title\n*   Author(s)\n*   Date\n*   Keywords\n*   Abstract (summary of the paper)\n*   Background information (related to the research topic)\n*   Research questions/hypotheses\n*   Materials/Equipment used\n*   Data analysis techniques\n*   Statistical results (tables",
               "figures",
               "and statistical tests)\n*   Interpretation of results\n*   Limitations of the study\n*   Future research directions\n*   Citations (references to other works)\n*   Data (potentially raw or processed data)\n*   Figures/Tables (visual representations of data)"
             ],
             "keyEntities": [
               {
                 "name": "[name]",
                 "type": "[type]",
                 "context": "[role]\" format.",
                 "isOwner": false
               }
             ],
             "role": "source"
           },
           {
             "documentId": "doc_1754658915966_7rvdl7exw",
             "documentName": "doc_1754658915966_7rvdl7exw",
             "documentType": "** Research Paper/Technical Report",
             "primaryEntity": "** Large Language Models (LLMs) in Aerospace Engineering",
             "structure": [
               "** research paper/technical report sections"
             ],
             "contentAreas": [
               "Here's an analysis of the document type and sample content",
               "outlining the likely structure and information types:\n\n**Main Content Areas/Sections:**\n\n*   Abstract/Introduction\n*   Literature Review\n*   Methodology/Approach\n*   Results/Findings\n*   Discussion\n*   Conclusion\n*   References/Bibliography\n*   Appendices (potentially)\n\n**Specific Types of Information:**\n\n*   Title (LLMS FOR ENGINEERING- TEACHIING MODELS TO DESIGN HIGH POWERED ROCKETS)\n*   Document Metadata (filename",
               "version number)\n*   Text (likely including technical terms",
               "equations",
               "and descriptions)\n*   Data (potentially numerical data",
               "graphs",
               "tables)\n*   Code (potentially code snippets related to the methodology)\n*   References (citations to other research papers)\n*   Keywords (implied by the title)"
             ],
             "keyEntities": [
               {
                 "name": "LLLMS",
                 "type": "Project/Technology",
                 "context": "Likely refers to Large Language Models (LLMs) being used for engineering and teaching models to design rockets.",
                 "isOwner": false
               },
               {
                 "name": "Rocketry",
                 "type": "Project/Field",
                 "context": "The overarching field the document focuses on - rocket design.",
                 "isOwner": false
               },
               {
                 "name": "Engineering",
                 "type": "Field",
                 "context": "The discipline that the document relates to.",
                 "isOwner": false
               }
             ],
             "role": "reference"
           }
         ],
         "relationships": [],
         "crossDocumentStrategy": "Process each document independently to prevent cross-contamination"
       },
       "sharedKnowledge": {},
       "filteredDocuments": 27,
       "reasoning": "Document analysis completed"
     }
   }

───────────────────────────────────

2. PlanningAgent ✅
   Type: PlanningAgent
   Status: completed
   Progress: 100%
   Stage: Completed
   Duration: 60040ms
   
   📤 Full Output:
   {
     "result": "success",
     "output": {
       "executionPlan": "Execution strategy created",
       "reasoning": "Planning completed"
     }
   }

───────────────────────────────────

3. PatternGenerator ✅
   Type: PatternGenerator
   Status: completed
   Progress: 100%
   Stage: Completed
   Duration: 2ms
   
   📤 Full Output:
   {
     "result": "success",
     "output": {
       "patterns": [
         {
           "description": "** Research Paper extraction pattern for ** Reinforcement Learning",
           "examples": [
             "Based on the provided document type (\"Research Paper\") and the sample content (\"Document metadata: GRPO_Paper.pdf\")",
             "here's a breakdown of the likely structure and information types:\n\n**Main Content Areas/Sections:**\n\n*   Abstract\n*   Introduction\n*   Literature Review\n*   Methodology\n*   Results\n*   Discussion\n*   Conclusion\n*   References/Bibliography\n*   Appendices (potentially)\n\n**Specific Types of Information Present:**\n\n*   Title\n*   Author(s)\n*   Date\n*   Keywords\n*   Abstract (summary of the paper)\n*   Background information (related to the research topic)\n*   Research questions/hypotheses\n*   Materials/Equipment used\n*   Data analysis techniques\n*   Statistical results (tables",
             "figures",
             "and statistical tests)\n*   Interpretation of results\n*   Limitations of the study\n*   Future research directions\n*   Citations (references to other works)\n*   Data (potentially raw or processed data)\n*   Figures/Tables (visual representations of data)"
           ],
           "extractionStrategy": "Extract Based on the provided document type (\"Research Paper\") and the sample content (\"Document metadata: GRPO_Paper.pdf\"), here's a breakdown of the likely structure and information types:\n\n**Main Content Areas/Sections:**\n\n*   Abstract\n*   Introduction\n*   Literature Review\n*   Methodology\n*   Results\n*   Discussion\n*   Conclusion\n*   References/Bibliography\n*   Appendices (potentially)\n\n**Specific Types of Information Present:**\n\n*   Title\n*   Author(s)\n*   Date\n*   Keywords\n*   Abstract (summary of the paper)\n*   Background information (related to the research topic)\n*   Research questions/hypotheses\n*   Materials/Equipment used\n*   Data analysis techniques\n*   Statistical results (tables, figures, and statistical tests)\n*   Interpretation of results\n*   Limitations of the study\n*   Future research directions\n*   Citations (references to other works)\n*   Data (potentially raw or processed data)\n*   Figures/Tables (visual representations of data) from doc_1754658663251_vpl5u967b",
           "confidence": 0.9
         },
         {
           "description": "** Research Paper/Technical Report extraction pattern for ** Large Language Models (LLMs) in Aerospace Engineering",
           "examples": [
             "Here's an analysis of the document type and sample content",
             "outlining the likely structure and information types:\n\n**Main Content Areas/Sections:**\n\n*   Abstract/Introduction\n*   Literature Review\n*   Methodology/Approach\n*   Results/Findings\n*   Discussion\n*   Conclusion\n*   References/Bibliography\n*   Appendices (potentially)\n\n**Specific Types of Information:**\n\n*   Title (LLMS FOR ENGINEERING- TEACHIING MODELS TO DESIGN HIGH POWERED ROCKETS)\n*   Document Metadata (filename",
             "version number)\n*   Text (likely including technical terms",
             "equations",
             "and descriptions)\n*   Data (potentially numerical data",
             "graphs",
             "tables)\n*   Code (potentially code snippets related to the methodology)\n*   References (citations to other research papers)\n*   Keywords (implied by the title)"
           ],
           "extractionStrategy": "Extract Here's an analysis of the document type and sample content, outlining the likely structure and information types:\n\n**Main Content Areas/Sections:**\n\n*   Abstract/Introduction\n*   Literature Review\n*   Methodology/Approach\n*   Results/Findings\n*   Discussion\n*   Conclusion\n*   References/Bibliography\n*   Appendices (potentially)\n\n**Specific Types of Information:**\n\n*   Title (LLMS FOR ENGINEERING- TEACHIING MODELS TO DESIGN HIGH POWERED ROCKETS)\n*   Document Metadata (filename, version number)\n*   Text (likely including technical terms, equations, and descriptions)\n*   Data (potentially numerical data, graphs, tables)\n*   Code (potentially code snippets related to the methodology)\n*   References (citations to other research papers)\n*   Keywords (implied by the title) from doc_1754658915966_7rvdl7exw",
           "confidence": 0.9
         },
         {
           "description": "Concept pattern for which",
           "examples": [],
           "extractionStrategy": "Extract information about which",
           "confidence": 0.8,
           "regexPattern": "/which[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi"
         },
         {
           "description": "Concept pattern for methods",
           "examples": [],
           "extractionStrategy": "Extract information about methods",
           "confidence": 0.8,
           "regexPattern": "/methods[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi"
         },
         {
           "description": "Ranking indicators",
           "examples": [],
           "extractionStrategy": "Extract ranking and comparison language",
           "confidence": 0.9,
           "regexPattern": "/(?:best|top|highest|superior|outperforms?|better than|exceeds)[^\\n]*/gi"
         },
         {
           "description": "Comparative metrics",
           "examples": [],
           "extractionStrategy": "Extract comparative performance data",
           "confidence": 0.9,
           "regexPattern": "/(?:vs|versus|compared to|against)[^\\n]*([\\d.]+%?)[^\\n]*/gi"
         }
       ],
       "patternCount": 6,
       "extractionStrategies": {
         "generatedPatterns": [
           "/which[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi",
           "/methods[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi",
           "/(?:best|top|highest|superior|outperforms?|better than|exceeds)[^\\n]*/gi",
           "/(?:vs|versus|compared to|against)[^\\n]*([\\d.]+%?)[^\\n]*/gi"
         ],
         "generationMethod": "planning_agent_strategy",
         "basedOnExtractionStrategy": true,
         "timestamp": 1754682072796,
         "agentSource": "PatternGenerator",
         "strategyUsed": {
           "documentType": "Generic Document",
           "queryIntent": "performance_ranking",
           "contentAreas": [],
           "patternCategories": {
             "people": [],
             "roles": [],
             "designations": [],
             "concepts": [
               "which",
               "methods"
             ],
             "methods": [],
             "data": []
           },
           "extractionTargets": [
             "content"
           ]
         }
       },
       "reasoning": "Pattern generation completed"
     }
   }

───────────────────────────────────

4. Extractor ✅
   Type: Extractor
   Status: completed
   Progress: 100%
   Stage: Completed
   Duration: 7ms
   
   📤 Full Output:
   {
     "result": "success",
     "output": {
       "extractedData": {
         "raw": [
           {
             "content": "which continues pre- training Deep Seek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. Deep Seek Math 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from Deep Seek Math 7B achieves 60.9% on MATH. The mathematical reasoning capability of Deep Seek Math is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO. Figure 1|Top1 accuracy of open-source models on the competition-level MATH benchmark (Hendrycks et al., 2021) without the use of external toolkits and voting techniques. ∗Core contributors. †Work done during internship at Deep Seek-AI. ar Xiv:2402.03300v3 [cs. CL] 27 Apr 2024 ----------------Page (0) Break---------------- 1. Introduction Large language models (LLM) have revolutionized the approach to",
             "value": "which continues pre- training Deep Seek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. Deep Seek Math 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from Deep Seek Math 7B achieves 60.9% on MATH. The mathematical reasoning capability of Deep Seek Math is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO. Figure 1|Top1 accuracy of open-source models on the competition-level MATH benchmark (Hendrycks et al., 2021) without the use of external toolkits and voting techniques. ∗Core contributors. †Work done during internship at Deep Seek-AI. ar Xiv:2402.03300v3 [cs. CL] 27 Apr 2024 ----------------Page (0) Break---------------- 1. Introduction Large language models (LLM) have revolutionized the approach to",
             "unit": "",
             "context": "which continues pre- training Deep Seek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. Deep Seek Math 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from Deep Seek Math 7B achieves 60.9% on MATH. The mathematical reasoning capability of Deep Seek Math is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO. Figure 1|Top1 accuracy of open-source models on the competition-level MATH benchmark (Hendrycks et al., 2021) without the use of external toolkits and voting techniques. ∗Core contributors. †Work done during internship at Deep Seek-AI. ar Xiv:2402.03300v3 [cs. CL] 27 Apr 2024 ----------------Page (0) Break---------------- 1. Introduction Large language models (LLM) have revolutionized the approach to",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_0_zl31nl",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/which[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi",
               "patternDescription": "Concept pattern for which",
               "fullMatch": "which continues pre- training Deep Seek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. Deep Seek Math 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from Deep Seek Math 7B achieves 60.9% on MATH. The mathematical reasoning capability of Deep Seek Math is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO. Figure 1|Top1 accuracy of open-source models on the competition-level MATH benchmark (Hendrycks et al., 2021) without the use of external toolkits and voting techniques. ∗Core contributors. †Work done during internship at Deep Seek-AI. ar Xiv:2402.03300v3 [cs. CL] 27 Apr 2024 ----------------Page (0) Break---------------- 1. Introduction Large language models (LLM) have revolutionized the approach to"
             }
           },
           {
             "content": "which shares the same framework as the Deep Seek LLMs (Deep Seek-AI, 2024), denoted as Deep Seek- LLM 1.3B. We separately train a model on each mathematical corpus for 150B tokens. All experiments are conducted using the efficient and light-weight HAI-LLM (High-flyer, 2023) training framework. Following the training practice of Deep Seek LLMs, we use the Adam W optimizer (Loshchilov and Hutter, 2017) with훽1=0.9,훽2=0.95, andweight_decay=0.1, along with a multi-step learning rate schedule where the learning rate reaches the peak after 2,000 warmup steps, decreases to its 31.6% after 80% of the training process, and further decreases to 10.0% of the peak after 90% of the training process. We set the maximum value of learning rate to 5.3e-4, and use a batch size of 4M tokens with a 4K context length. Math Corpus Size English Benchmarks Chinese Benchmarks GSM8K MATH OCW SATMMLUSTEMCMATHGaokao Math Cloze Gaokao Math QA No Math Training N/A 2.9% 3.0% 2.9% 15.6% 19.5% 12.3% 0.8% 17.9% Math Pile 8.9B 2.7% 3.3% 2.2% 12.5% 15.7% 1.2% 0.0% 2.8% Open Web Math 13.6B 11.5% 8.9% 3.7% 31.3% 29.6% 16.8% 0.0% 14.2% Proof-Pile-2 51.9B 14.3% 11.2% 3.7% 43.8% 29.2% 19.9% 5.1% 11.7% Deep Seek Math Corpus120.2B 23.8% 13.6% 4.8% 56.3% 33.1% 41.5% 5.9% 23.6% Table 1|Performance of Deep Seek-LLM 1.3B trained on different mathematical corpora, evalu-",
             "value": "which shares the same framework as the Deep Seek LLMs (Deep Seek-AI, 2024), denoted as Deep Seek- LLM 1.3B. We separately train a model on each mathematical corpus for 150B tokens. All experiments are conducted using the efficient and light-weight HAI-LLM (High-flyer, 2023) training framework. Following the training practice of Deep Seek LLMs, we use the Adam W optimizer (Loshchilov and Hutter, 2017) with훽1=0.9,훽2=0.95, andweight_decay=0.1, along with a multi-step learning rate schedule where the learning rate reaches the peak after 2,000 warmup steps, decreases to its 31.6% after 80% of the training process, and further decreases to 10.0% of the peak after 90% of the training process. We set the maximum value of learning rate to 5.3e-4, and use a batch size of 4M tokens with a 4K context length. Math Corpus Size English Benchmarks Chinese Benchmarks GSM8K MATH OCW SATMMLUSTEMCMATHGaokao Math Cloze Gaokao Math QA No Math Training N/A 2.9% 3.0% 2.9% 15.6% 19.5% 12.3% 0.8% 17.9% Math Pile 8.9B 2.7% 3.3% 2.2% 12.5% 15.7% 1.2% 0.0% 2.8% Open Web Math 13.6B 11.5% 8.9% 3.7% 31.3% 29.6% 16.8% 0.0% 14.2% Proof-Pile-2 51.9B 14.3% 11.2% 3.7% 43.8% 29.2% 19.9% 5.1% 11.7% Deep Seek Math Corpus120.2B 23.8% 13.6% 4.8% 56.3% 33.1% 41.5% 5.9% 23.6% Table 1|Performance of Deep Seek-LLM 1.3B trained on different mathematical corpora, evalu-",
             "unit": "",
             "context": "which shares the same framework as the Deep Seek LLMs (Deep Seek-AI, 2024), denoted as Deep Seek- LLM 1.3B. We separately train a model on each mathematical corpus for 150B tokens. All experiments are conducted using the efficient and light-weight HAI-LLM (High-flyer, 2023) training framework. Following the training practice of Deep Seek LLMs, we use the Adam W optimizer (Loshchilov and Hutter, 2017) with훽1=0.9,훽2=0.95, andweight_decay=0.1, along with a multi-step learning rate schedule where the learning rate reaches the peak after 2,000 warmup steps, decreases to its 31.6% after 80% of the training process, and further decreases to 10.0% of the peak after 90% of the training process. We set the maximum value of learning rate to 5.3e-4, and use a batch size of 4M tokens with a 4K context length. Math Corpus Size English Benchmarks Chinese Benchmarks GSM8K MATH OCW SATMMLUSTEMCMATHGaokao Math Cloze Gaokao Math QA No Math Training N/A 2.9% 3.0% 2.9% 15.6% 19.5% 12.3% 0.8% 17.9% Math Pile 8.9B 2.7% 3.3% 2.2% 12.5% 15.7% 1.2% 0.0% 2.8% Open Web Math 13.6B 11.5% 8.9% 3.7% 31.3% 29.6% 16.8% 0.0% 14.2% Proof-Pile-2 51.9B 14.3% 11.2% 3.7% 43.8% 29.2% 19.9% 5.1% 11.7% Deep Seek Math Corpus120.2B 23.8% 13.6% 4.8% 56.3% 33.1% 41.5% 5.9% 23.6% Table 1|Performance of Deep Seek-LLM 1.3B trained on different mathematical corpora, evalu-",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_12_fm1222",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/which[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi",
               "patternDescription": "Concept pattern for which",
               "fullMatch": "which shares the same framework as the Deep Seek LLMs (Deep Seek-AI, 2024), denoted as Deep Seek- LLM 1.3B. We separately train a model on each mathematical corpus for 150B tokens. All experiments are conducted using the efficient and light-weight HAI-LLM (High-flyer, 2023) training framework. Following the training practice of Deep Seek LLMs, we use the Adam W optimizer (Loshchilov and Hutter, 2017) with훽1=0.9,훽2=0.95, andweight_decay=0.1, along with a multi-step learning rate schedule where the learning rate reaches the peak after 2,000 warmup steps, decreases to its 31.6% after 80% of the training process, and further decreases to 10.0% of the peak after 90% of the training process. We set the maximum value of learning rate to 5.3e-4, and use a batch size of 4M tokens with a 4K context length. Math Corpus Size English Benchmarks Chinese Benchmarks GSM8K MATH OCW SATMMLUSTEMCMATHGaokao Math Cloze Gaokao Math QA No Math Training N/A 2.9% 3.0% 2.9% 15.6% 19.5% 12.3% 0.8% 17.9% Math Pile 8.9B 2.7% 3.3% 2.2% 12.5% 15.7% 1.2% 0.0% 2.8% Open Web Math 13.6B 11.5% 8.9% 3.7% 31.3% 29.6% 16.8% 0.0% 14.2% Proof-Pile-2 51.9B 14.3% 11.2% 3.7% 43.8% 29.2% 19.9% 5.1% 11.7% Deep Seek Math Corpus120.2B 23.8% 13.6% 4.8% 56.3% 33.1% 41.5% 5.9% 23.6% Table 1|Performance of Deep Seek-LLM 1.3B trained on different mathematical corpora, evalu-"
             }
           },
           {
             "content": "which underwent math training on Proof-Pile-2 (Azerbayev et al., 2023)). Notably, on the competition- level MATH dataset, Deep Seek Math-Base surpasses existing open-source base models by over",
             "value": "which underwent math training on Proof-Pile-2 (Azerbayev et al., 2023)). Notably, on the competition- level MATH dataset, Deep Seek Math-Base surpasses existing open-source base models by over",
             "unit": "",
             "context": "which underwent math training on Proof-Pile-2 (Azerbayev et al., 2023)). Notably, on the competition- level MATH dataset, Deep Seek Math-Base surpasses existing open-source base models by over",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_15_g5me6s",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/which[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi",
               "patternDescription": "Concept pattern for which",
               "fullMatch": "which underwent math training on Proof-Pile-2 (Azerbayev et al., 2023)). Notably, on the competition- level MATH dataset, Deep Seek Math-Base surpasses existing open-source base models by over"
             }
           },
           {
             "content": "which is to generate a formal proof based on an informal statement, a formal counterpart of the statement, and an informal proof. We evaluate on mini F2F (Zheng et al., 2021), a bench- mark for formal Olympiad-level mathematics, and generate a formal proof in Isabelle for each problem with few-shot prompting. Following Jiang et al. (2022), we leverage models to generate proof sketches, and execute the off-the-shelf automated prover Sledgehammer (Paulson, 2010) to fill in the missing details. As shown in Table 3, Deep Seek Math-Base 7B demonstrates strong performance in proof autoformalization. Model Size MMLU BBH Human Eval (Pass@1) MBPP (Pass@1) Mistral 7B62.4%55.7% 28.0% 41.4% Deep Seek-Coder-Base-v1.5†7B 42.9% 42.9% 40.2% 52.6% Deep Seek-Coder-Base-v1.5 7B 49.1% 55.2%43.2% 60.4% Deep Seek Math-Base 7B 54.9%59.5%40.9% 52.6% Table 4|Evaluation on natural language understanding, reasoning, and code benchmarks. Deep Seek-Coder-Base-v1.5†is the checkpoint right before learning rate decay, which is used to train Deep Seek Math-Base. On MMLU and BBH, we use few-shot chain-of-thought prompting. On Human Eval and MBPP, we evaluate model performance under the zero-shot setting and a few-shot setting, respectively. Natural Language Understanding, Reasoning, and Code We evaluate model performance of natural language understanding on MMLU (Hendrycks et al., 2020), reasoning on BBH (Suzgun et al., 2022), and coding capabilities on Human Eval (Chen et al., 2021) and MBPP (Austin et al., 9 ----------------Page (8) Break---------------- 2021). As shown in Table 4, Deep Seek Math-Base 7B exhibits significant enhancements in per- formance on MMLU and BBH over its",
             "value": "which is to generate a formal proof based on an informal statement, a formal counterpart of the statement, and an informal proof. We evaluate on mini F2F (Zheng et al., 2021), a bench- mark for formal Olympiad-level mathematics, and generate a formal proof in Isabelle for each problem with few-shot prompting. Following Jiang et al. (2022), we leverage models to generate proof sketches, and execute the off-the-shelf automated prover Sledgehammer (Paulson, 2010) to fill in the missing details. As shown in Table 3, Deep Seek Math-Base 7B demonstrates strong performance in proof autoformalization. Model Size MMLU BBH Human Eval (Pass@1) MBPP (Pass@1) Mistral 7B62.4%55.7% 28.0% 41.4% Deep Seek-Coder-Base-v1.5†7B 42.9% 42.9% 40.2% 52.6% Deep Seek-Coder-Base-v1.5 7B 49.1% 55.2%43.2% 60.4% Deep Seek Math-Base 7B 54.9%59.5%40.9% 52.6% Table 4|Evaluation on natural language understanding, reasoning, and code benchmarks. Deep Seek-Coder-Base-v1.5†is the checkpoint right before learning rate decay, which is used to train Deep Seek Math-Base. On MMLU and BBH, we use few-shot chain-of-thought prompting. On Human Eval and MBPP, we evaluate model performance under the zero-shot setting and a few-shot setting, respectively. Natural Language Understanding, Reasoning, and Code We evaluate model performance of natural language understanding on MMLU (Hendrycks et al., 2020), reasoning on BBH (Suzgun et al., 2022), and coding capabilities on Human Eval (Chen et al., 2021) and MBPP (Austin et al., 9 ----------------Page (8) Break---------------- 2021). As shown in Table 4, Deep Seek Math-Base 7B exhibits significant enhancements in per- formance on MMLU and BBH over its",
             "unit": "",
             "context": "which is to generate a formal proof based on an informal statement, a formal counterpart of the statement, and an informal proof. We evaluate on mini F2F (Zheng et al., 2021), a bench- mark for formal Olympiad-level mathematics, and generate a formal proof in Isabelle for each problem with few-shot prompting. Following Jiang et al. (2022), we leverage models to generate proof sketches, and execute the off-the-shelf automated prover Sledgehammer (Paulson, 2010) to fill in the missing details. As shown in Table 3, Deep Seek Math-Base 7B demonstrates strong performance in proof autoformalization. Model Size MMLU BBH Human Eval (Pass@1) MBPP (Pass@1) Mistral 7B62.4%55.7% 28.0% 41.4% Deep Seek-Coder-Base-v1.5†7B 42.9% 42.9% 40.2% 52.6% Deep Seek-Coder-Base-v1.5 7B 49.1% 55.2%43.2% 60.4% Deep Seek Math-Base 7B 54.9%59.5%40.9% 52.6% Table 4|Evaluation on natural language understanding, reasoning, and code benchmarks. Deep Seek-Coder-Base-v1.5†is the checkpoint right before learning rate decay, which is used to train Deep Seek Math-Base. On MMLU and BBH, we use few-shot chain-of-thought prompting. On Human Eval and MBPP, we evaluate model performance under the zero-shot setting and a few-shot setting, respectively. Natural Language Understanding, Reasoning, and Code We evaluate model performance of natural language understanding on MMLU (Hendrycks et al., 2020), reasoning on BBH (Suzgun et al., 2022), and coding capabilities on Human Eval (Chen et al., 2021) and MBPP (Austin et al., 9 ----------------Page (8) Break---------------- 2021). As shown in Table 4, Deep Seek Math-Base 7B exhibits significant enhancements in per- formance on MMLU and BBH over its",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_18_4hmkky",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/which[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi",
               "patternDescription": "Concept pattern for which",
               "fullMatch": "which is to generate a formal proof based on an informal statement, a formal counterpart of the statement, and an informal proof. We evaluate on mini F2F (Zheng et al., 2021), a bench- mark for formal Olympiad-level mathematics, and generate a formal proof in Isabelle for each problem with few-shot prompting. Following Jiang et al. (2022), we leverage models to generate proof sketches, and execute the off-the-shelf automated prover Sledgehammer (Paulson, 2010) to fill in the missing details. As shown in Table 3, Deep Seek Math-Base 7B demonstrates strong performance in proof autoformalization. Model Size MMLU BBH Human Eval (Pass@1) MBPP (Pass@1) Mistral 7B62.4%55.7% 28.0% 41.4% Deep Seek-Coder-Base-v1.5†7B 42.9% 42.9% 40.2% 52.6% Deep Seek-Coder-Base-v1.5 7B 49.1% 55.2%43.2% 60.4% Deep Seek Math-Base 7B 54.9%59.5%40.9% 52.6% Table 4|Evaluation on natural language understanding, reasoning, and code benchmarks. Deep Seek-Coder-Base-v1.5†is the checkpoint right before learning rate decay, which is used to train Deep Seek Math-Base. On MMLU and BBH, we use few-shot chain-of-thought prompting. On Human Eval and MBPP, we evaluate model performance under the zero-shot setting and a few-shot setting, respectively. Natural Language Understanding, Reasoning, and Code We evaluate model performance of natural language understanding on MMLU (Hendrycks et al., 2020), reasoning on BBH (Suzgun et al., 2022), and coding capabilities on Human Eval (Chen et al., 2021) and MBPP (Austin et al., 9 ----------------Page (8) Break---------------- 2021). As shown in Table 4, Deep Seek Math-Base 7B exhibits significant enhancements in per- formance on MMLU and BBH over its"
             }
           },
           {
             "content": "which have undergone a series of alignment procedures. •Open-source modelsinclude: general models like (1) Deep Seek-LLM-Chat 67B (Deep Seek- AI, 2024), (2) Qwen 72B (Bai et al., 2023), (3) Sea LLM-v2 7B (Nguyen et al., 2023), and (4) 2https://openai.com/blog/chatgpt-plugins#code-interpreter 3https://x.ai/model-card 4https://www.baichuan-ai.com 5https://open.bigmodel.cn/dev/api#glm-4 10 ----------------Page (9) Break---------------- Chat GLM3 6B (Chat GLM3 Team, 2023), as well as models with enhancements in mathemat- ics including (5) Intern LM2-Math 20B6which builds on Intern LM2 and underwent math training followed by instruction tuning, (6) Math-Shepherd-Mistral 7B which applys PPO training (Schulman et al., 2017) to Mistral 7B (Jiang et al., 2023) with a process-supervised reward model, (7) the Wizard Math series (Luo et al., 2023) which improves mathematical reasoning in Mistral 7B and Llama-2 70B (Touvron et al., 2023) using evolve-instruct (i.e., a version of instruction tuning that uses AI-evolved instructions) and PPO training with training problems primarily sourced from GSM8K and MATH, (8) Meta Math 70B (Yu et al., 2023) which is Llama-2 70B fine-tuned on an augmented version of GSM8K and MATH, (9) To RA 34B Gou et al. (2023) which is Code Llama 34B fine-tuned to do tool-integrated mathematical reasoning, (10) MAmmo TH 70B (Yue et al., 2023) which is Llama-2 70B instruction-tuned on Math Instruct. As shown in Table 5, under the evaluation setting where tool use",
             "value": "which have undergone a series of alignment procedures. •Open-source modelsinclude: general models like (1) Deep Seek-LLM-Chat 67B (Deep Seek- AI, 2024), (2) Qwen 72B (Bai et al., 2023), (3) Sea LLM-v2 7B (Nguyen et al., 2023), and (4) 2https://openai.com/blog/chatgpt-plugins#code-interpreter 3https://x.ai/model-card 4https://www.baichuan-ai.com 5https://open.bigmodel.cn/dev/api#glm-4 10 ----------------Page (9) Break---------------- Chat GLM3 6B (Chat GLM3 Team, 2023), as well as models with enhancements in mathemat- ics including (5) Intern LM2-Math 20B6which builds on Intern LM2 and underwent math training followed by instruction tuning, (6) Math-Shepherd-Mistral 7B which applys PPO training (Schulman et al., 2017) to Mistral 7B (Jiang et al., 2023) with a process-supervised reward model, (7) the Wizard Math series (Luo et al., 2023) which improves mathematical reasoning in Mistral 7B and Llama-2 70B (Touvron et al., 2023) using evolve-instruct (i.e., a version of instruction tuning that uses AI-evolved instructions) and PPO training with training problems primarily sourced from GSM8K and MATH, (8) Meta Math 70B (Yu et al., 2023) which is Llama-2 70B fine-tuned on an augmented version of GSM8K and MATH, (9) To RA 34B Gou et al. (2023) which is Code Llama 34B fine-tuned to do tool-integrated mathematical reasoning, (10) MAmmo TH 70B (Yue et al., 2023) which is Llama-2 70B instruction-tuned on Math Instruct. As shown in Table 5, under the evaluation setting where tool use",
             "unit": "",
             "context": "which have undergone a series of alignment procedures. •Open-source modelsinclude: general models like (1) Deep Seek-LLM-Chat 67B (Deep Seek- AI, 2024), (2) Qwen 72B (Bai et al., 2023), (3) Sea LLM-v2 7B (Nguyen et al., 2023), and (4) 2https://openai.com/blog/chatgpt-plugins#code-interpreter 3https://x.ai/model-card 4https://www.baichuan-ai.com 5https://open.bigmodel.cn/dev/api#glm-4 10 ----------------Page (9) Break---------------- Chat GLM3 6B (Chat GLM3 Team, 2023), as well as models with enhancements in mathemat- ics including (5) Intern LM2-Math 20B6which builds on Intern LM2 and underwent math training followed by instruction tuning, (6) Math-Shepherd-Mistral 7B which applys PPO training (Schulman et al., 2017) to Mistral 7B (Jiang et al., 2023) with a process-supervised reward model, (7) the Wizard Math series (Luo et al., 2023) which improves mathematical reasoning in Mistral 7B and Llama-2 70B (Touvron et al., 2023) using evolve-instruct (i.e., a version of instruction tuning that uses AI-evolved instructions) and PPO training with training problems primarily sourced from GSM8K and MATH, (8) Meta Math 70B (Yu et al., 2023) which is Llama-2 70B fine-tuned on an augmented version of GSM8K and MATH, (9) To RA 34B Gou et al. (2023) which is Code Llama 34B fine-tuned to do tool-integrated mathematical reasoning, (10) MAmmo TH 70B (Yue et al., 2023) which is Llama-2 70B instruction-tuned on Math Instruct. As shown in Table 5, under the evaluation setting where tool use",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_21_5k3xdq",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/which[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi",
               "patternDescription": "Concept pattern for which",
               "fullMatch": "which have undergone a series of alignment procedures. •Open-source modelsinclude: general models like (1) Deep Seek-LLM-Chat 67B (Deep Seek- AI, 2024), (2) Qwen 72B (Bai et al., 2023), (3) Sea LLM-v2 7B (Nguyen et al., 2023), and (4) 2https://openai.com/blog/chatgpt-plugins#code-interpreter 3https://x.ai/model-card 4https://www.baichuan-ai.com 5https://open.bigmodel.cn/dev/api#glm-4 10 ----------------Page (9) Break---------------- Chat GLM3 6B (Chat GLM3 Team, 2023), as well as models with enhancements in mathemat- ics including (5) Intern LM2-Math 20B6which builds on Intern LM2 and underwent math training followed by instruction tuning, (6) Math-Shepherd-Mistral 7B which applys PPO training (Schulman et al., 2017) to Mistral 7B (Jiang et al., 2023) with a process-supervised reward model, (7) the Wizard Math series (Luo et al., 2023) which improves mathematical reasoning in Mistral 7B and Llama-2 70B (Touvron et al., 2023) using evolve-instruct (i.e., a version of instruction tuning that uses AI-evolved instructions) and PPO training with training problems primarily sourced from GSM8K and MATH, (8) Meta Math 70B (Yu et al., 2023) which is Llama-2 70B fine-tuned on an augmented version of GSM8K and MATH, (9) To RA 34B Gou et al. (2023) which is Code Llama 34B fine-tuned to do tool-integrated mathematical reasoning, (10) MAmmo TH 70B (Yue et al., 2023) which is Llama-2 70B instruction-tuned on Math Instruct. As shown in Table 5, under the evaluation setting where tool use"
             }
           },
           {
             "content": "which is guaranteed to be positive. 4.1.2. Outcome Supervision RL with GRPO Formally, for each question푞, a group of outputs{표1,표2,···,표퐺}are sampled from the old policy model휋휃표푙푑. A reward model is then used to score the outputs, yielding퐺rewards r={푟1,푟2,···,푟퐺}correspondingly. Subsequently, these rewards are normalized by subtracting the group average and dividing by the group standard deviation. Outcome supervision provides the normalized reward at the end of each output표푖and",
             "value": "which is guaranteed to be positive. 4.1.2. Outcome Supervision RL with GRPO Formally, for each question푞, a group of outputs{표1,표2,···,표퐺}are sampled from the old policy model휋휃표푙푑. A reward model is then used to score the outputs, yielding퐺rewards r={푟1,푟2,···,푟퐺}correspondingly. Subsequently, these rewards are normalized by subtracting the group average and dividing by the group standard deviation. Outcome supervision provides the normalized reward at the end of each output표푖and",
             "unit": "",
             "context": "which is guaranteed to be positive. 4.1.2. Outcome Supervision RL with GRPO Formally, for each question푞, a group of outputs{표1,표2,···,표퐺}are sampled from the old policy model휋휃표푙푑. A reward model is then used to score the outputs, yielding퐺rewards r={푟1,푟2,···,푟퐺}correspondingly. Subsequently, these rewards are normalized by subtracting the group average and dividing by the group standard deviation. Outcome supervision provides the normalized reward at the end of each output표푖and",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_27_yntgx4",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/which[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi",
               "patternDescription": "Concept pattern for which",
               "fullMatch": "which is guaranteed to be positive. 4.1.2. Outcome Supervision RL with GRPO Formally, for each question푞, a group of outputs{표1,표2,···,표퐺}are sampled from the old policy model휋휃표푙푑. A reward model is then used to score the outputs, yielding퐺rewards r={푟1,푟2,···,푟퐺}correspondingly. Subsequently, these rewards are normalized by subtracting the group average and dividing by the group standard deviation. Outcome supervision provides the normalized reward at the end of each output표푖and"
             }
           },
           {
             "content": "which are scientific ar Xiv papers; •Ar Xiv-Red Pajama(Computer, 2023): the entirety of ar Xiv La Te X files with preambles, comments, macros, and bibliographies removed, totaling 28.0B tokens. In our experiments, we separately train Deep Seek-LLM 1.3B for 150B tokens and Deep Seek- Coder-Base-v1.5 7B for 40B tokens on each ar Xiv corpus. It seems that ar Xiv papers are ineffective in improving mathematical reasoning. When trained on a ar Xiv-only corpus, both models dis- play no notable improvements or even deterioration across various mathematical benchmarks of different complexities employed in this study. These benchmarks include quantitative reasoning datasets like GSM8K and MATH (Table 8), multiple-choice challenges like MMLU-STEM (Table 8), and formal mathematics like mini F2F (Table 9). However, this conclusion has its limitations and should be taken with a grain of salt. We have not yet studied: •The impact of ar Xiv tokens on specific math-related tasks not included in this research, such as informalization of theorems which is to convert formal statements or proofs to their informal versions; • The effect of ar Xiv tokens when combined with other types of data; • Whether the benefits of ar Xiv papers would manifest themselves at a larger model scale. Thus, further exploration is required, which we leave for future studies. 5.2.",
             "value": "which are scientific ar Xiv papers; •Ar Xiv-Red Pajama(Computer, 2023): the entirety of ar Xiv La Te X files with preambles, comments, macros, and bibliographies removed, totaling 28.0B tokens. In our experiments, we separately train Deep Seek-LLM 1.3B for 150B tokens and Deep Seek- Coder-Base-v1.5 7B for 40B tokens on each ar Xiv corpus. It seems that ar Xiv papers are ineffective in improving mathematical reasoning. When trained on a ar Xiv-only corpus, both models dis- play no notable improvements or even deterioration across various mathematical benchmarks of different complexities employed in this study. These benchmarks include quantitative reasoning datasets like GSM8K and MATH (Table 8), multiple-choice challenges like MMLU-STEM (Table 8), and formal mathematics like mini F2F (Table 9). However, this conclusion has its limitations and should be taken with a grain of salt. We have not yet studied: •The impact of ar Xiv tokens on specific math-related tasks not included in this research, such as informalization of theorems which is to convert formal statements or proofs to their informal versions; • The effect of ar Xiv tokens when combined with other types of data; • Whether the benefits of ar Xiv papers would manifest themselves at a larger model scale. Thus, further exploration is required, which we leave for future studies. 5.2.",
             "unit": "",
             "context": "which are scientific ar Xiv papers; •Ar Xiv-Red Pajama(Computer, 2023): the entirety of ar Xiv La Te X files with preambles, comments, macros, and bibliographies removed, totaling 28.0B tokens. In our experiments, we separately train Deep Seek-LLM 1.3B for 150B tokens and Deep Seek- Coder-Base-v1.5 7B for 40B tokens on each ar Xiv corpus. It seems that ar Xiv papers are ineffective in improving mathematical reasoning. When trained on a ar Xiv-only corpus, both models dis- play no notable improvements or even deterioration across various mathematical benchmarks of different complexities employed in this study. These benchmarks include quantitative reasoning datasets like GSM8K and MATH (Table 8), multiple-choice challenges like MMLU-STEM (Table 8), and formal mathematics like mini F2F (Table 9). However, this conclusion has its limitations and should be taken with a grain of salt. We have not yet studied: •The impact of ar Xiv tokens on specific math-related tasks not included in this research, such as informalization of theorems which is to convert formal statements or proofs to their informal versions; • The effect of ar Xiv tokens when combined with other types of data; • Whether the benefits of ar Xiv papers would manifest themselves at a larger model scale. Thus, further exploration is required, which we leave for future studies. 5.2.",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_36_9zlx5u",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/which[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi",
               "patternDescription": "Concept pattern for which",
               "fullMatch": "which are scientific ar Xiv papers; •Ar Xiv-Red Pajama(Computer, 2023): the entirety of ar Xiv La Te X files with preambles, comments, macros, and bibliographies removed, totaling 28.0B tokens. In our experiments, we separately train Deep Seek-LLM 1.3B for 150B tokens and Deep Seek- Coder-Base-v1.5 7B for 40B tokens on each ar Xiv corpus. It seems that ar Xiv papers are ineffective in improving mathematical reasoning. When trained on a ar Xiv-only corpus, both models dis- play no notable improvements or even deterioration across various mathematical benchmarks of different complexities employed in this study. These benchmarks include quantitative reasoning datasets like GSM8K and MATH (Table 8), multiple-choice challenges like MMLU-STEM (Table 8), and formal mathematics like mini F2F (Table 9). However, this conclusion has its limitations and should be taken with a grain of salt. We have not yet studied: •The impact of ar Xiv tokens on specific math-related tasks not included in this research, such as informalization of theorems which is to convert formal statements or proofs to their informal versions; • The effect of ar Xiv tokens when combined with other types of data; • Whether the benefits of ar Xiv papers would manifest themselves at a larger model scale. Thus, further exploration is required, which we leave for future studies. 5.2."
             }
           },
           {
             "content": "which can notably improve mathematical reasoning capabilities with less memory consumption. The experiment results show that GRPO is effective even if Deep Seek Math-Instruct 7B has reached a high score on benchmarks. We also provide a unified paradigm to understand a series of methods and summarize several potential directions for more effective reinforcement learning. Although Deep Seek Math achieves impressive scores on quantitative reasoning benchmarks, its capability on geometry and theorem-proof are relatively weaker than closed models. For instance, in our dry run, the model cannot handle problems related to triangles and ellipses, which may indicate data selection bias in pre-training and fine-tuning. In addition, restricted by the model scale, Deep Seek Math is worse than GPT-4 on few-shot capability. GPT-4 could improve its performance with few-shot inputs, while Deep Seek Math shows similar performance in zero-shot and few-shot evaluation. In the future, we will further improve our engineered data selection pipeline to construct more high-quality pre-trained corpus. In addition, we will explore the potential directions (Section 5.2.3) for more effective reinforcement learning of LLMs. 7https://github.com/openai/prm800k/issues/12#issuecomment-1728491852 22 ----------------Page (21) Break---------------- References R. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A.",
             "value": "which can notably improve mathematical reasoning capabilities with less memory consumption. The experiment results show that GRPO is effective even if Deep Seek Math-Instruct 7B has reached a high score on benchmarks. We also provide a unified paradigm to understand a series of methods and summarize several potential directions for more effective reinforcement learning. Although Deep Seek Math achieves impressive scores on quantitative reasoning benchmarks, its capability on geometry and theorem-proof are relatively weaker than closed models. For instance, in our dry run, the model cannot handle problems related to triangles and ellipses, which may indicate data selection bias in pre-training and fine-tuning. In addition, restricted by the model scale, Deep Seek Math is worse than GPT-4 on few-shot capability. GPT-4 could improve its performance with few-shot inputs, while Deep Seek Math shows similar performance in zero-shot and few-shot evaluation. In the future, we will further improve our engineered data selection pipeline to construct more high-quality pre-trained corpus. In addition, we will explore the potential directions (Section 5.2.3) for more effective reinforcement learning of LLMs. 7https://github.com/openai/prm800k/issues/12#issuecomment-1728491852 22 ----------------Page (21) Break---------------- References R. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A.",
             "unit": "",
             "context": "which can notably improve mathematical reasoning capabilities with less memory consumption. The experiment results show that GRPO is effective even if Deep Seek Math-Instruct 7B has reached a high score on benchmarks. We also provide a unified paradigm to understand a series of methods and summarize several potential directions for more effective reinforcement learning. Although Deep Seek Math achieves impressive scores on quantitative reasoning benchmarks, its capability on geometry and theorem-proof are relatively weaker than closed models. For instance, in our dry run, the model cannot handle problems related to triangles and ellipses, which may indicate data selection bias in pre-training and fine-tuning. In addition, restricted by the model scale, Deep Seek Math is worse than GPT-4 on few-shot capability. GPT-4 could improve its performance with few-shot inputs, while Deep Seek Math shows similar performance in zero-shot and few-shot evaluation. In the future, we will further improve our engineered data selection pipeline to construct more high-quality pre-trained corpus. In addition, we will explore the potential directions (Section 5.2.3) for more effective reinforcement learning of LLMs. 7https://github.com/openai/prm800k/issues/12#issuecomment-1728491852 22 ----------------Page (21) Break---------------- References R. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A.",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_45_eidwqr",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/which[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi",
               "patternDescription": "Concept pattern for which",
               "fullMatch": "which can notably improve mathematical reasoning capabilities with less memory consumption. The experiment results show that GRPO is effective even if Deep Seek Math-Instruct 7B has reached a high score on benchmarks. We also provide a unified paradigm to understand a series of methods and summarize several potential directions for more effective reinforcement learning. Although Deep Seek Math achieves impressive scores on quantitative reasoning benchmarks, its capability on geometry and theorem-proof are relatively weaker than closed models. For instance, in our dry run, the model cannot handle problems related to triangles and ellipses, which may indicate data selection bias in pre-training and fine-tuning. In addition, restricted by the model scale, Deep Seek Math is worse than GPT-4 on few-shot capability. GPT-4 could improve its performance with few-shot inputs, while Deep Seek Math shows similar performance in zero-shot and few-shot evaluation. In the future, we will further improve our engineered data selection pipeline to construct more high-quality pre-trained corpus. In addition, we will explore the potential directions (Section 5.2.3) for more effective reinforcement learning of LLMs. 7https://github.com/openai/prm800k/issues/12#issuecomment-1728491852 22 ----------------Page (21) Break---------------- References R. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A."
             }
           },
           {
             "content": "which is computed by applying Generalized Advantage Estimation (GAE) (Schulman et al., 2015), based on the rewards{푟≥푡}and a learned value function푉휓. A.1.6. Group Relative Policy Optimization (GRPO) The objective of GRPO is (assume휋휃표푙푑=휋휃for simplified analysis): J퐺푅푃푂(휃)=E[푞∼푃푠푓푡(푄),{표푖}퐺푖=1∼휋휃표푙푑(푂|푞)] 1 퐺 퐺∑︁ 푖=1 1 |표푖| |표푖|∑︁ 푡=1 휋 휃(표푖,푡|푞,표푖,<푡) 휋휃표푙푑(표푖,푡|푞,표푖,<푡)ˆ퐴푖,푡−훽( 휋푟푒푓(표푖,푡|푞,표푖,<푡) 휋휃(표푖,푡|푞,표푖,<푡)−log 휋푟푒푓(표푖,푡|푞,표푖,<푡) 휋휃(표푖,푡|푞,표푖,<푡)−1) .(19) 29 ----------------Page (28) Break---------------- The gradient of J퐺푅푃푂(휃)is: ∇휃J퐺푅푃푂(휃)=E[푞∼푃푠푓푡(푄),{표푖}퐺푖=1∼휋휃표푙푑(푂|푞)] 1 퐺 퐺∑︁ 푖=1 1 |표푖| |표푖|∑︁ 푡=1 ˆ퐴푖,푡+훽 휋 푟푒푓(표푖,푡|표푖,<푡) 휋휃(표푖,푡|표푖,<푡)−1 ∇휃log휋휃(표푖,푡|푞,표푖,<푡).(20) Data Source: question in SFT dataset with outputs sampled from policy model. Reward Function: reward model. Gradient Coefficient: 퐺퐶퐺푅푃푂(푞,표,푡,휋휃푟푚)=ˆ퐴푖,푡+훽 휋 푟푒푓(표푖,푡|표푖,<푡) 휋휃(표푖,푡|표푖,<푡)−1 , (21) whereˆ퐴푖,푡is computed based on the group reward scores. 30 ----------------Page (29) Break----------------",
             "value": "which is computed by applying Generalized Advantage Estimation (GAE) (Schulman et al., 2015), based on the rewards{푟≥푡}and a learned value function푉휓. A.1.6. Group Relative Policy Optimization (GRPO) The objective of GRPO is (assume휋휃표푙푑=휋휃for simplified analysis): J퐺푅푃푂(휃)=E[푞∼푃푠푓푡(푄),{표푖}퐺푖=1∼휋휃표푙푑(푂|푞)] 1 퐺 퐺∑︁ 푖=1 1 |표푖| |표푖|∑︁ 푡=1 휋 휃(표푖,푡|푞,표푖,<푡) 휋휃표푙푑(표푖,푡|푞,표푖,<푡)ˆ퐴푖,푡−훽( 휋푟푒푓(표푖,푡|푞,표푖,<푡) 휋휃(표푖,푡|푞,표푖,<푡)−log 휋푟푒푓(표푖,푡|푞,표푖,<푡) 휋휃(표푖,푡|푞,표푖,<푡)−1) .(19) 29 ----------------Page (28) Break---------------- The gradient of J퐺푅푃푂(휃)is: ∇휃J퐺푅푃푂(휃)=E[푞∼푃푠푓푡(푄),{표푖}퐺푖=1∼휋휃표푙푑(푂|푞)] 1 퐺 퐺∑︁ 푖=1 1 |표푖| |표푖|∑︁ 푡=1 ˆ퐴푖,푡+훽 휋 푟푒푓(표푖,푡|표푖,<푡) 휋휃(표푖,푡|표푖,<푡)−1 ∇휃log휋휃(표푖,푡|푞,표푖,<푡).(20) Data Source: question in SFT dataset with outputs sampled from policy model. Reward Function: reward model. Gradient Coefficient: 퐺퐶퐺푅푃푂(푞,표,푡,휋휃푟푚)=ˆ퐴푖,푡+훽 휋 푟푒푓(표푖,푡|표푖,<푡) 휋휃(표푖,푡|표푖,<푡)−1 , (21) whereˆ퐴푖,푡is computed based on the group reward scores. 30 ----------------Page (29) Break----------------",
             "unit": "",
             "context": "which is computed by applying Generalized Advantage Estimation (GAE) (Schulman et al., 2015), based on the rewards{푟≥푡}and a learned value function푉휓. A.1.6. Group Relative Policy Optimization (GRPO) The objective of GRPO is (assume휋휃표푙푑=휋휃for simplified analysis): J퐺푅푃푂(휃)=E[푞∼푃푠푓푡(푄),{표푖}퐺푖=1∼휋휃표푙푑(푂|푞)] 1 퐺 퐺∑︁ 푖=1 1 |표푖| |표푖|∑︁ 푡=1 휋 휃(표푖,푡|푞,표푖,<푡) 휋휃표푙푑(표푖,푡|푞,표푖,<푡)ˆ퐴푖,푡−훽( 휋푟푒푓(표푖,푡|푞,표푖,<푡) 휋휃(표푖,푡|푞,표푖,<푡)−log 휋푟푒푓(표푖,푡|푞,표푖,<푡) 휋휃(표푖,푡|푞,표푖,<푡)−1) .(19) 29 ----------------Page (28) Break---------------- The gradient of J퐺푅푃푂(휃)is: ∇휃J퐺푅푃푂(휃)=E[푞∼푃푠푓푡(푄),{표푖}퐺푖=1∼휋휃표푙푑(푂|푞)] 1 퐺 퐺∑︁ 푖=1 1 |표푖| |표푖|∑︁ 푡=1 ˆ퐴푖,푡+훽 휋 푟푒푓(표푖,푡|표푖,<푡) 휋휃(표푖,푡|표푖,<푡)−1 ∇휃log휋휃(표푖,푡|푞,표푖,<푡).(20) Data Source: question in SFT dataset with outputs sampled from policy model. Reward Function: reward model. Gradient Coefficient: 퐺퐶퐺푅푃푂(푞,표,푡,휋휃푟푚)=ˆ퐴푖,푡+훽 휋 푟푒푓(표푖,푡|표푖,<푡) 휋휃(표푖,푡|표푖,<푡)−1 , (21) whereˆ퐴푖,푡is computed based on the group reward scores. 30 ----------------Page (29) Break----------------",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_59_jy3aq8",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/which[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi",
               "patternDescription": "Concept pattern for which",
               "fullMatch": "which is computed by applying Generalized Advantage Estimation (GAE) (Schulman et al., 2015), based on the rewards{푟≥푡}and a learned value function푉휓. A.1.6. Group Relative Policy Optimization (GRPO) The objective of GRPO is (assume휋휃표푙푑=휋휃for simplified analysis): J퐺푅푃푂(휃)=E[푞∼푃푠푓푡(푄),{표푖}퐺푖=1∼휋휃표푙푑(푂|푞)] 1 퐺 퐺∑︁ 푖=1 1 |표푖| |표푖|∑︁ 푡=1 휋 휃(표푖,푡|푞,표푖,<푡) 휋휃표푙푑(표푖,푡|푞,표푖,<푡)ˆ퐴푖,푡−훽( 휋푟푒푓(표푖,푡|푞,표푖,<푡) 휋휃(표푖,푡|푞,표푖,<푡)−log 휋푟푒푓(표푖,푡|푞,표푖,<푡) 휋휃(표푖,푡|푞,표푖,<푡)−1) .(19) 29 ----------------Page (28) Break---------------- The gradient of J퐺푅푃푂(휃)is: ∇휃J퐺푅푃푂(휃)=E[푞∼푃푠푓푡(푄),{표푖}퐺푖=1∼휋휃표푙푑(푂|푞)] 1 퐺 퐺∑︁ 푖=1 1 |표푖| |표푖|∑︁ 푡=1 ˆ퐴푖,푡+훽 휋 푟푒푓(표푖,푡|표푖,<푡) 휋휃(표푖,푡|표푖,<푡)−1 ∇휃log휋휃(표푖,푡|푞,표푖,<푡).(20) Data Source: question in SFT dataset with outputs sampled from policy model. Reward Function: reward model. Gradient Coefficient: 퐺퐶퐺푅푃푂(푞,표,푡,휋휃푟푚)=ˆ퐴푖,푡+훽 휋 푟푒푓(표푖,푡|표푖,<푡) 휋휃(표푖,푡|표푖,<푡)−1 , (21) whereˆ퐴푖,푡is computed based on the group reward scores. 30 ----------------Page (29) Break----------------"
             }
           },
           {
             "content": "methods, such as Rejection Sampling Fine-Tuning (RFT) (Yuan et al., 2023a), Direct Preference Optimization (DPO) (Rafailov et al., 2023), PPO and GRPO. Based on such a unified paradigm, we find that all these methods are conceptualized as either direct or simplified RL techniques. We also conduct extensive experiments, e.g., online v.s. offline training, outcome v.s. process supervision, single-turn v.s. iterative RL and so on, 2 ----------------Page (1) Break---------------- to deeply investigate the essential elements of this paradigm.",
             "value": "methods, such as Rejection Sampling Fine-Tuning (RFT) (Yuan et al., 2023a), Direct Preference Optimization (DPO) (Rafailov et al., 2023), PPO and GRPO. Based on such a unified paradigm, we find that all these methods are conceptualized as either direct or simplified RL techniques. We also conduct extensive experiments, e.g., online v.s. offline training, outcome v.s. process supervision, single-turn v.s. iterative RL and so on, 2 ----------------Page (1) Break---------------- to deeply investigate the essential elements of this paradigm.",
             "unit": "",
             "context": "methods, such as Rejection Sampling Fine-Tuning (RFT) (Yuan et al., 2023a), Direct Preference Optimization (DPO) (Rafailov et al., 2023), PPO and GRPO. Based on such a unified paradigm, we find that all these methods are conceptualized as either direct or simplified RL techniques. We also conduct extensive experiments, e.g., online v.s. offline training, outcome v.s. process supervision, single-turn v.s. iterative RL and so on, 2 ----------------Page (1) Break---------------- to deeply investigate the essential elements of this paradigm.",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_3_s02fqv",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/methods[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi",
               "patternDescription": "Concept pattern for methods",
               "fullMatch": "methods, such as Rejection Sampling Fine-Tuning (RFT) (Yuan et al., 2023a), Direct Preference Optimization (DPO) (Rafailov et al., 2023), PPO and GRPO. Based on such a unified paradigm, we find that all these methods are conceptualized as either direct or simplified RL techniques. We also conduct extensive experiments, e.g., online v.s. offline training, outcome v.s. process supervision, single-turn v.s. iterative RL and so on, 2 ----------------Page (1) Break---------------- to deeply investigate the essential elements of this paradigm."
             }
           },
           {
             "content": "methods in Table 10. Please refer to Appendix A.1 for a more detailed derivation process. Observation about Data Source We divide the data source into two categories, online sam- pling, and offline sampling. Online sampling denotes that the training data is from the explo- ration results of the real-time training policy model, while offline sampling denotes that the 19 ----------------Page (18) Break---------------- 013002300330043005300Steps 83 84 85 86 87 88 89 Acc (%) GSM8K 013002300330043005300Steps 47 48 49 50 51 52 Acc (%) MATH Iteration-0Iteration-1Iteration-2 Figure 6|Performance of iterative reinforcement learning with Deep Seek Math-Instruct 7B on two benchmarks. training data is from the sampling results of the initial SFT model. RFT and DPO follow the offline style, while Online RFT and GRPO follow the online style. As shown in Figure 5, we find that the Online RFT significantly outperforms RFT on two benchmarks. Specifically, Online RFT is comparable to RFT in the early stage of training but gains an absolute advantage in the later stage, demonstrating the superiority of online training. This is intuitive, as in the initial stage, the actor and the SFT model exhibit close resemblance, with the sampled data revealing only minor differences. In the later stage, however, the data sampled from the actor will exhibit more significant differences, and real-time data sampling will offer greater advantages. Observation about Gradient Coefficient",
             "value": "methods in Table 10. Please refer to Appendix A.1 for a more detailed derivation process. Observation about Data Source We divide the data source into two categories, online sam- pling, and offline sampling. Online sampling denotes that the training data is from the explo- ration results of the real-time training policy model, while offline sampling denotes that the 19 ----------------Page (18) Break---------------- 013002300330043005300Steps 83 84 85 86 87 88 89 Acc (%) GSM8K 013002300330043005300Steps 47 48 49 50 51 52 Acc (%) MATH Iteration-0Iteration-1Iteration-2 Figure 6|Performance of iterative reinforcement learning with Deep Seek Math-Instruct 7B on two benchmarks. training data is from the sampling results of the initial SFT model. RFT and DPO follow the offline style, while Online RFT and GRPO follow the online style. As shown in Figure 5, we find that the Online RFT significantly outperforms RFT on two benchmarks. Specifically, Online RFT is comparable to RFT in the early stage of training but gains an absolute advantage in the later stage, demonstrating the superiority of online training. This is intuitive, as in the initial stage, the actor and the SFT model exhibit close resemblance, with the sampled data revealing only minor differences. In the later stage, however, the data sampled from the actor will exhibit more significant differences, and real-time data sampling will offer greater advantages. Observation about Gradient Coefficient",
             "unit": "",
             "context": "methods in Table 10. Please refer to Appendix A.1 for a more detailed derivation process. Observation about Data Source We divide the data source into two categories, online sam- pling, and offline sampling. Online sampling denotes that the training data is from the explo- ration results of the real-time training policy model, while offline sampling denotes that the 19 ----------------Page (18) Break---------------- 013002300330043005300Steps 83 84 85 86 87 88 89 Acc (%) GSM8K 013002300330043005300Steps 47 48 49 50 51 52 Acc (%) MATH Iteration-0Iteration-1Iteration-2 Figure 6|Performance of iterative reinforcement learning with Deep Seek Math-Instruct 7B on two benchmarks. training data is from the sampling results of the initial SFT model. RFT and DPO follow the offline style, while Online RFT and GRPO follow the online style. As shown in Figure 5, we find that the Online RFT significantly outperforms RFT on two benchmarks. Specifically, Online RFT is comparable to RFT in the early stage of training but gains an absolute advantage in the later stage, demonstrating the superiority of online training. This is intuitive, as in the initial stage, the actor and the SFT model exhibit close resemblance, with the sampled data revealing only minor differences. In the later stage, however, the data sampled from the actor will exhibit more significant differences, and real-time data sampling will offer greater advantages. Observation about Gradient Coefficient",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_39_rkt5wa",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/methods[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi",
               "patternDescription": "Concept pattern for methods",
               "fullMatch": "methods in Table 10. Please refer to Appendix A.1 for a more detailed derivation process. Observation about Data Source We divide the data source into two categories, online sam- pling, and offline sampling. Online sampling denotes that the training data is from the explo- ration results of the real-time training policy model, while offline sampling denotes that the 19 ----------------Page (18) Break---------------- 013002300330043005300Steps 83 84 85 86 87 88 89 Acc (%) GSM8K 013002300330043005300Steps 47 48 49 50 51 52 Acc (%) MATH Iteration-0Iteration-1Iteration-2 Figure 6|Performance of iterative reinforcement learning with Deep Seek Math-Instruct 7B on two benchmarks. training data is from the sampling results of the initial SFT model. RFT and DPO follow the offline style, while Online RFT and GRPO follow the online style. As shown in Figure 5, we find that the Online RFT significantly outperforms RFT on two benchmarks. Specifically, Online RFT is comparable to RFT in the early stage of training but gains an absolute advantage in the later stage, demonstrating the superiority of online training. This is intuitive, as in the initial stage, the actor and the SFT model exhibit close resemblance, with the sampled data revealing only minor differences. In the later stage, however, the data sampled from the actor will exhibit more significant differences, and real-time data sampling will offer greater advantages. Observation about Gradient Coefficient"
             }
           },
           {
             "content": "methods. Within this paradigm, all methods are conceptualized as either direct or simplified RL techniques. As summarized in Equation 5, there exist three key components: Data Source, Algorithm, and Reward Function. We provide some potential future directions about the three components. Data Source Data source is the raw material of all training methods. In the context of RL, we specifically refer to the data source as the unlabeled questions with the outputs sampled from the policy model. In this paper, we only use the questions from the instruction tuning stage and a naive nucleus sampling to sample outputs. We think this is a potential reason that our RL pipeline only improves the Maj@K performance. In the future, we will explore our RL pipeline on out-of-distribution question prompts, in conjunction withadvanced sampling (decoding) strategies, like those based on",
             "value": "methods. Within this paradigm, all methods are conceptualized as either direct or simplified RL techniques. As summarized in Equation 5, there exist three key components: Data Source, Algorithm, and Reward Function. We provide some potential future directions about the three components. Data Source Data source is the raw material of all training methods. In the context of RL, we specifically refer to the data source as the unlabeled questions with the outputs sampled from the policy model. In this paper, we only use the questions from the instruction tuning stage and a naive nucleus sampling to sample outputs. We think this is a potential reason that our RL pipeline only improves the Maj@K performance. In the future, we will explore our RL pipeline on out-of-distribution question prompts, in conjunction withadvanced sampling (decoding) strategies, like those based on",
             "unit": "",
             "context": "methods. Within this paradigm, all methods are conceptualized as either direct or simplified RL techniques. As summarized in Equation 5, there exist three key components: Data Source, Algorithm, and Reward Function. We provide some potential future directions about the three components. Data Source Data source is the raw material of all training methods. In the context of RL, we specifically refer to the data source as the unlabeled questions with the outputs sampled from the policy model. In this paper, we only use the questions from the instruction tuning stage and a naive nucleus sampling to sample outputs. We think this is a potential reason that our RL pipeline only improves the Maj@K performance. In the future, we will explore our RL pipeline on out-of-distribution question prompts, in conjunction withadvanced sampling (decoding) strategies, like those based on",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_42_gjzrxq",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/methods[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi",
               "patternDescription": "Concept pattern for methods",
               "fullMatch": "methods. Within this paradigm, all methods are conceptualized as either direct or simplified RL techniques. As summarized in Equation 5, there exist three key components: Data Source, Algorithm, and Reward Function. We provide some potential future directions about the three components. Data Source Data source is the raw material of all training methods. In the context of RL, we specifically refer to the data source as the unlabeled questions with the outputs sampled from the policy model. In this paper, we only use the questions from the instruction tuning stage and a naive nucleus sampling to sample outputs. We think this is a potential reason that our RL pipeline only improves the Maj@K performance. In the future, we will explore our RL pipeline on out-of-distribution question prompts, in conjunction withadvanced sampling (decoding) strategies, like those based on"
             }
           },
           {
             "content": "methods and summarize several potential directions for more effective reinforcement learning. Although Deep Seek Math achieves impressive scores on quantitative reasoning benchmarks, its capability on geometry and theorem-proof are relatively weaker than closed models. For instance, in our dry run, the model cannot handle problems related to triangles and ellipses, which may indicate data selection bias in pre-training and fine-tuning. In addition, restricted by the model scale, Deep Seek Math is worse than GPT-4 on few-shot capability. GPT-4 could improve its performance with few-shot inputs, while Deep Seek Math shows similar performance in zero-shot and few-shot evaluation. In the future, we will further improve our engineered data selection pipeline to construct more high-quality pre-trained corpus. In addition, we will explore the potential directions (Section 5.2.3) for more effective reinforcement learning of LLMs. 7https://github.com/openai/prm800k/issues/12#issuecomment-1728491852 22 ----------------Page (21) Break---------------- References R. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A.",
             "value": "methods and summarize several potential directions for more effective reinforcement learning. Although Deep Seek Math achieves impressive scores on quantitative reasoning benchmarks, its capability on geometry and theorem-proof are relatively weaker than closed models. For instance, in our dry run, the model cannot handle problems related to triangles and ellipses, which may indicate data selection bias in pre-training and fine-tuning. In addition, restricted by the model scale, Deep Seek Math is worse than GPT-4 on few-shot capability. GPT-4 could improve its performance with few-shot inputs, while Deep Seek Math shows similar performance in zero-shot and few-shot evaluation. In the future, we will further improve our engineered data selection pipeline to construct more high-quality pre-trained corpus. In addition, we will explore the potential directions (Section 5.2.3) for more effective reinforcement learning of LLMs. 7https://github.com/openai/prm800k/issues/12#issuecomment-1728491852 22 ----------------Page (21) Break---------------- References R. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A.",
             "unit": "",
             "context": "methods and summarize several potential directions for more effective reinforcement learning. Although Deep Seek Math achieves impressive scores on quantitative reasoning benchmarks, its capability on geometry and theorem-proof are relatively weaker than closed models. For instance, in our dry run, the model cannot handle problems related to triangles and ellipses, which may indicate data selection bias in pre-training and fine-tuning. In addition, restricted by the model scale, Deep Seek Math is worse than GPT-4 on few-shot capability. GPT-4 could improve its performance with few-shot inputs, while Deep Seek Math shows similar performance in zero-shot and few-shot evaluation. In the future, we will further improve our engineered data selection pipeline to construct more high-quality pre-trained corpus. In addition, we will explore the potential directions (Section 5.2.3) for more effective reinforcement learning of LLMs. 7https://github.com/openai/prm800k/issues/12#issuecomment-1728491852 22 ----------------Page (21) Break---------------- References R. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A.",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_45_eidwqr",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/methods[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi",
               "patternDescription": "Concept pattern for methods",
               "fullMatch": "methods and summarize several potential directions for more effective reinforcement learning. Although Deep Seek Math achieves impressive scores on quantitative reasoning benchmarks, its capability on geometry and theorem-proof are relatively weaker than closed models. For instance, in our dry run, the model cannot handle problems related to triangles and ellipses, which may indicate data selection bias in pre-training and fine-tuning. In addition, restricted by the model scale, Deep Seek Math is worse than GPT-4 on few-shot capability. GPT-4 could improve its performance with few-shot inputs, while Deep Seek Math shows similar performance in zero-shot and few-shot evaluation. In the future, we will further improve our engineered data selection pipeline to construct more high-quality pre-trained corpus. In addition, we will explore the potential directions (Section 5.2.3) for more effective reinforcement learning of LLMs. 7https://github.com/openai/prm800k/issues/12#issuecomment-1728491852 22 ----------------Page (21) Break---------------- References R. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A."
             }
           },
           {
             "content": "methods. In subsequent iterations, we augmented the prompt with the model’s full output including reasoning for the previous solution and the comprehensive performance metrics. We include all previous solutions not just previous attempt. For the Target Altitude Challenge, feedback included maximum apogee achieved, structural integrity status, and final cost. For the Precision Landing Challenge, we substituted landing position for apogee information. This approach enabled models to learn from previous design attempts while maintaining consistent evaluation conditions. 3.5 Human Baseline To establish a comparative baseline, we recruited an individual with multiple years of experience in university-level rocketry competitions similar to those our benchmarks are based off. This participant completed identical design tasks with the same interface, components, and feedback mechanisms used by the models.",
             "value": "methods. In subsequent iterations, we augmented the prompt with the model’s full output including reasoning for the previous solution and the comprehensive performance metrics. We include all previous solutions not just previous attempt. For the Target Altitude Challenge, feedback included maximum apogee achieved, structural integrity status, and final cost. For the Precision Landing Challenge, we substituted landing position for apogee information. This approach enabled models to learn from previous design attempts while maintaining consistent evaluation conditions. 3.5 Human Baseline To establish a comparative baseline, we recruited an individual with multiple years of experience in university-level rocketry competitions similar to those our benchmarks are based off. This participant completed identical design tasks with the same interface, components, and feedback mechanisms used by the models.",
             "unit": "",
             "context": "methods. In subsequent iterations, we augmented the prompt with the model’s full output including reasoning for the previous solution and the comprehensive performance metrics. We include all previous solutions not just previous attempt. For the Target Altitude Challenge, feedback included maximum apogee achieved, structural integrity status, and final cost. For the Precision Landing Challenge, we substituted landing position for apogee information. This approach enabled models to learn from previous design attempts while maintaining consistent evaluation conditions. 3.5 Human Baseline To establish a comparative baseline, we recruited an individual with multiple years of experience in university-level rocketry competitions similar to those our benchmarks are based off. This participant completed identical design tasks with the same interface, components, and feedback mechanisms used by the models.",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658915966_7rvdl7exw_1754658932262_9_8i3iox",
             "sourceDocument": "LLMS FOR ENGINEERING- TEACHING MODELS TO DESIGN HIGH POWERED ROCKETS_2504.19394v2.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/methods[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi",
               "patternDescription": "Concept pattern for methods",
               "fullMatch": "methods. In subsequent iterations, we augmented the prompt with the model’s full output including reasoning for the previous solution and the comprehensive performance metrics. We include all previous solutions not just previous attempt. For the Target Altitude Challenge, feedback included maximum apogee achieved, structural integrity status, and final cost. For the Precision Landing Challenge, we substituted landing position for apogee information. This approach enabled models to learn from previous design attempts while maintaining consistent evaluation conditions. 3.5 Human Baseline To establish a comparative baseline, we recruited an individual with multiple years of experience in university-level rocketry competitions similar to those our benchmarks are based off. This participant completed identical design tasks with the same interface, components, and feedback mechanisms used by the models."
             }
           },
           {
             "content": "methods still plateau below human level performance. We observe that reasoning-focused models (o1) outperform foundation models (GPT-4o), suggesting that further scaling of the reasoning model paradigm could yield additional performance improvements. Test-time reinforcement learning (TTRL) represents another promising axis for scaling compute to enhance performance. Beyond providing an incremental boost in specific domains, TTRL offers a methodology for extending performance boundaries in individual domains, potentially previewing future capabilities of large language models. 6.2 Why use LLMs for RL? Despite impressive successes like Alpha Go and Alpha Zero",
             "value": "methods still plateau below human level performance. We observe that reasoning-focused models (o1) outperform foundation models (GPT-4o), suggesting that further scaling of the reasoning model paradigm could yield additional performance improvements. Test-time reinforcement learning (TTRL) represents another promising axis for scaling compute to enhance performance. Beyond providing an incremental boost in specific domains, TTRL offers a methodology for extending performance boundaries in individual domains, potentially previewing future capabilities of large language models. 6.2 Why use LLMs for RL? Despite impressive successes like Alpha Go and Alpha Zero",
             "unit": "",
             "context": "methods still plateau below human level performance. We observe that reasoning-focused models (o1) outperform foundation models (GPT-4o), suggesting that further scaling of the reasoning model paradigm could yield additional performance improvements. Test-time reinforcement learning (TTRL) represents another promising axis for scaling compute to enhance performance. Beyond providing an incremental boost in specific domains, TTRL offers a methodology for extending performance boundaries in individual domains, potentially previewing future capabilities of large language models. 6.2 Why use LLMs for RL? Despite impressive successes like Alpha Go and Alpha Zero",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658915966_7rvdl7exw_1754658932262_18_njs105",
             "sourceDocument": "LLMS FOR ENGINEERING- TEACHING MODELS TO DESIGN HIGH POWERED ROCKETS_2504.19394v2.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/methods[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi",
               "patternDescription": "Concept pattern for methods",
               "fullMatch": "methods still plateau below human level performance. We observe that reasoning-focused models (o1) outperform foundation models (GPT-4o), suggesting that further scaling of the reasoning model paradigm could yield additional performance improvements. Test-time reinforcement learning (TTRL) represents another promising axis for scaling compute to enhance performance. Beyond providing an incremental boost in specific domains, TTRL offers a methodology for extending performance boundaries in individual domains, potentially previewing future capabilities of large language models. 6.2 Why use LLMs for RL? Despite impressive successes like Alpha Go and Alpha Zero"
             }
           },
           {
             "content": "methods can bypass conventional safeguards 6.5 Conclusion Our research demonstrates that reinforcement learning applied to LLMs creates a powerful paradigm for engineering optimization that surpasses both foundation models and human expert performance. While state-of-the-art LLMs show strong baseline engineering knowledge, they consistently plateau below human capabilities when iteratively refining designs. In contrast, RL-trained models achieved performance breakthroughs in both target altitude and precision landing challenges using only a modest parameter architecture. This approach addresses core limitations of both traditional RL (sample inefficiency) and foundation models (limited",
             "value": "methods can bypass conventional safeguards 6.5 Conclusion Our research demonstrates that reinforcement learning applied to LLMs creates a powerful paradigm for engineering optimization that surpasses both foundation models and human expert performance. While state-of-the-art LLMs show strong baseline engineering knowledge, they consistently plateau below human capabilities when iteratively refining designs. In contrast, RL-trained models achieved performance breakthroughs in both target altitude and precision landing challenges using only a modest parameter architecture. This approach addresses core limitations of both traditional RL (sample inefficiency) and foundation models (limited",
             "unit": "",
             "context": "methods can bypass conventional safeguards 6.5 Conclusion Our research demonstrates that reinforcement learning applied to LLMs creates a powerful paradigm for engineering optimization that surpasses both foundation models and human expert performance. While state-of-the-art LLMs show strong baseline engineering knowledge, they consistently plateau below human capabilities when iteratively refining designs. In contrast, RL-trained models achieved performance breakthroughs in both target altitude and precision landing challenges using only a modest parameter architecture. This approach addresses core limitations of both traditional RL (sample inefficiency) and foundation models (limited",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658915966_7rvdl7exw_1754658932262_21_5sjplo",
             "sourceDocument": "LLMS FOR ENGINEERING- TEACHING MODELS TO DESIGN HIGH POWERED ROCKETS_2504.19394v2.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/methods[^\\n]*(?:is|are|involves|includes|means|refers)[^\\n]*/gi",
               "patternDescription": "Concept pattern for methods",
               "fullMatch": "methods can bypass conventional safeguards 6.5 Conclusion Our research demonstrates that reinforcement learning applied to LLMs creates a powerful paradigm for engineering optimization that surpasses both foundation models and human expert performance. While state-of-the-art LLMs show strong baseline engineering knowledge, they consistently plateau below human capabilities when iteratively refining designs. In contrast, RL-trained models achieved performance breakthroughs in both target altitude and precision landing challenges using only a modest parameter architecture. This approach addresses core limitations of both traditional RL (sample inefficiency) and foundation models (limited"
             }
           },
           {
             "content": "Top1 accuracy of open-source models on the competition-level MATH benchmark (Hendrycks et al., 2021) without the use of external toolkits and voting techniques. ∗Core contributors. †Work done during internship at Deep Seek-AI. ar Xiv:2402.03300v3 [cs. CL] 27 Apr 2024 ----------------Page (0) Break---------------- 1. Introduction Large language models (LLM) have revolutionized the approach to",
             "value": "Top1 accuracy of open-source models on the competition-level MATH benchmark (Hendrycks et al., 2021) without the use of external toolkits and voting techniques. ∗Core contributors. †Work done during internship at Deep Seek-AI. ar Xiv:2402.03300v3 [cs. CL] 27 Apr 2024 ----------------Page (0) Break---------------- 1. Introduction Large language models (LLM) have revolutionized the approach to",
             "unit": "",
             "context": "Top1 accuracy of open-source models on the competition-level MATH benchmark (Hendrycks et al., 2021) without the use of external toolkits and voting techniques. ∗Core contributors. †Work done during internship at Deep Seek-AI. ar Xiv:2402.03300v3 [cs. CL] 27 Apr 2024 ----------------Page (0) Break---------------- 1. Introduction Large language models (LLM) have revolutionized the approach to",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_0_zl31nl",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/(?:best|top|highest|superior|outperforms?|better than|exceeds)[^\\n]*/gi",
               "patternDescription": "Ranking indicators",
               "fullMatch": "Top1 accuracy of open-source models on the competition-level MATH benchmark (Hendrycks et al., 2021) without the use of external toolkits and voting techniques. ∗Core contributors. †Work done during internship at Deep Seek-AI. ar Xiv:2402.03300v3 [cs. CL] 27 Apr 2024 ----------------Page (0) Break---------------- 1. Introduction Large language models (LLM) have revolutionized the approach to"
             }
           },
           {
             "content": "superior on Chinese benchmarks, likely because we don’t follow previous works (Azerbayev et al., 2023; Lewkowycz et al., 2022a) to collect",
             "value": "superior on Chinese benchmarks, likely because we don’t follow previous works (Azerbayev et al., 2023; Lewkowycz et al., 2022a) to collect",
             "unit": "",
             "context": "superior on Chinese benchmarks, likely because we don’t follow previous works (Azerbayev et al., 2023; Lewkowycz et al., 2022a) to collect",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_6_5vghv4",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/(?:best|top|highest|superior|outperforms?|better than|exceeds)[^\\n]*/gi",
               "patternDescription": "Ranking indicators",
               "fullMatch": "superior on Chinese benchmarks, likely because we don’t follow previous works (Azerbayev et al., 2023; Lewkowycz et al., 2022a) to collect"
             }
           },
           {
             "content": "top-ranking ones. The volume of data preserved is assessed through pre-training experiments on the top 40B, 80B, 120B, and 160B tokens. In the first iteration, we choose to keep the top 40B tokens. After the first iteration of data collection, numerous mathematical web pages remain un- collected, mainly because the fast Text model is trained on a set of positive examples that lacks sufficient diversity. We therefore identify additional mathematical web sources to enrich the seed corpus, so that we can optimize the fast Text model. Specifically, we first organize the entire Com- mon Crawl into disjoint domains; a domain is defined as web pages sharing the same",
             "value": "top-ranking ones. The volume of data preserved is assessed through pre-training experiments on the top 40B, 80B, 120B, and 160B tokens. In the first iteration, we choose to keep the top 40B tokens. After the first iteration of data collection, numerous mathematical web pages remain un- collected, mainly because the fast Text model is trained on a set of positive examples that lacks sufficient diversity. We therefore identify additional mathematical web sources to enrich the seed corpus, so that we can optimize the fast Text model. Specifically, we first organize the entire Com- mon Crawl into disjoint domains; a domain is defined as web pages sharing the same",
             "unit": "",
             "context": "top-ranking ones. The volume of data preserved is assessed through pre-training experiments on the top 40B, 80B, 120B, and 160B tokens. In the first iteration, we choose to keep the top 40B tokens. After the first iteration of data collection, numerous mathematical web pages remain un- collected, mainly because the fast Text model is trained on a set of positive examples that lacks sufficient diversity. We therefore identify additional mathematical web sources to enrich the seed corpus, so that we can optimize the fast Text model. Specifically, we first organize the entire Com- mon Crawl into disjoint domains; a domain is defined as web pages sharing the same",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_9_lwjnk5",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/(?:best|top|highest|superior|outperforms?|better than|exceeds)[^\\n]*/gi",
               "patternDescription": "Ranking indicators",
               "fullMatch": "top-ranking ones. The volume of data preserved is assessed through pre-training experiments on the top 40B, 80B, 120B, and 160B tokens. In the first iteration, we choose to keep the top 40B tokens. After the first iteration of data collection, numerous mathematical web pages remain un- collected, mainly because the fast Text model is trained on a set of positive examples that lacks sufficient diversity. We therefore identify additional mathematical web sources to enrich the seed corpus, so that we can optimize the fast Text model. Specifically, we first organize the entire Com- mon Crawl into disjoint domains; a domain is defined as web pages sharing the same"
             }
           },
           {
             "content": "Top1 scores. Deep Seek Math-RL 7B beats all open- source models from 7B to 70B, as well as the majority of closed-source models. Although Deep Seek Math-RL 7B is only further trained on chain-of-thought-format instruction tuning data of GSM8K and MATH, it improves over Deep Seek Math-Instruct 7B on all benchmarks. 12 ----------------Page (11) Break----------------",
             "value": "Top1 scores. Deep Seek Math-RL 7B beats all open- source models from 7B to 70B, as well as the majority of closed-source models. Although Deep Seek Math-RL 7B is only further trained on chain-of-thought-format instruction tuning data of GSM8K and MATH, it improves over Deep Seek Math-Instruct 7B on all benchmarks. 12 ----------------Page (11) Break----------------",
             "unit": "",
             "context": "Top1 scores. Deep Seek Math-RL 7B beats all open- source models from 7B to 70B, as well as the majority of closed-source models. Although Deep Seek Math-RL 7B is only further trained on chain-of-thought-format instruction tuning data of GSM8K and MATH, it improves over Deep Seek Math-Instruct 7B on all benchmarks. 12 ----------------Page (11) Break----------------",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_24_ovwacs",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/(?:best|top|highest|superior|outperforms?|better than|exceeds)[^\\n]*/gi",
               "patternDescription": "Ranking indicators",
               "fullMatch": "Top1 scores. Deep Seek Math-RL 7B beats all open- source models from 7B to 70B, as well as the majority of closed-source models. Although Deep Seek Math-RL 7B is only further trained on chain-of-thought-format instruction tuning data of GSM8K and MATH, it improves over Deep Seek Math-Instruct 7B on all benchmarks. 12 ----------------Page (11) Break----------------"
             }
           },
           {
             "content": "outperforms Deep Seek Math-Instruct 7B across all evaluation metrics, showcasing the effectiveness of reinforcement learning. 5. Discussion In this section, we will share our findings in pre-training and RL experiments. 5.1. Lessons Learnt in Pre-Training We first share our experience in pre-training. Unless otherwise specified, we will adhere",
             "value": "outperforms Deep Seek Math-Instruct 7B across all evaluation metrics, showcasing the effectiveness of reinforcement learning. 5. Discussion In this section, we will share our findings in pre-training and RL experiments. 5.1. Lessons Learnt in Pre-Training We first share our experience in pre-training. Unless otherwise specified, we will adhere",
             "unit": "",
             "context": "outperforms Deep Seek Math-Instruct 7B across all evaluation metrics, showcasing the effectiveness of reinforcement learning. 5. Discussion In this section, we will share our findings in pre-training and RL experiments. 5.1. Lessons Learnt in Pre-Training We first share our experience in pre-training. Unless otherwise specified, we will adhere",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_30_5akyr7",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/(?:best|top|highest|superior|outperforms?|better than|exceeds)[^\\n]*/gi",
               "patternDescription": "Ranking indicators",
               "fullMatch": "outperforms Deep Seek Math-Instruct 7B across all evaluation metrics, showcasing the effectiveness of reinforcement learning. 5. Discussion In this section, we will share our findings in pre-training and RL experiments. 5.1. Lessons Learnt in Pre-Training We first share our experience in pre-training. Unless otherwise specified, we will adhere"
             }
           },
           {
             "content": "outperforms RFT on two benchmarks. Specifically, Online RFT is comparable to RFT in the early stage of training but gains an absolute advantage in the later stage, demonstrating the superiority of online training. This is intuitive, as in the initial stage, the actor and the SFT model exhibit close resemblance, with the sampled data revealing only minor differences. In the later stage, however, the data sampled from the actor will exhibit more significant differences, and real-time data sampling will offer greater advantages. Observation about Gradient Coefficient",
             "value": "outperforms RFT on two benchmarks. Specifically, Online RFT is comparable to RFT in the early stage of training but gains an absolute advantage in the later stage, demonstrating the superiority of online training. This is intuitive, as in the initial stage, the actor and the SFT model exhibit close resemblance, with the sampled data revealing only minor differences. In the later stage, however, the data sampled from the actor will exhibit more significant differences, and real-time data sampling will offer greater advantages. Observation about Gradient Coefficient",
             "unit": "",
             "context": "outperforms RFT on two benchmarks. Specifically, Online RFT is comparable to RFT in the early stage of training but gains an absolute advantage in the later stage, demonstrating the superiority of online training. This is intuitive, as in the initial stage, the actor and the SFT model exhibit close resemblance, with the sampled data revealing only minor differences. In the later stage, however, the data sampled from the actor will exhibit more significant differences, and real-time data sampling will offer greater advantages. Observation about Gradient Coefficient",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_39_rkt5wa",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/(?:best|top|highest|superior|outperforms?|better than|exceeds)[^\\n]*/gi",
               "patternDescription": "Ranking indicators",
               "fullMatch": "outperforms RFT on two benchmarks. Specifically, Online RFT is comparable to RFT in the early stage of training but gains an absolute advantage in the later stage, demonstrating the superiority of online training. This is intuitive, as in the initial stage, the actor and the SFT model exhibit close resemblance, with the sampled data revealing only minor differences. In the later stage, however, the data sampled from the actor will exhibit more significant differences, and real-time data sampling will offer greater advantages. Observation about Gradient Coefficient"
             }
           },
           {
             "content": "Top K rather than the enhancement of fundamental capabilities. Similarly, (Wang et al., 2023a) identified a misalignment problemin reasoning tasks within the SFT model, showing that the reasoning performance of SFT models can be improved through a series of preference alignment strategies (Song et al., 2023; Wang et al., 2023a; Yuan et al., 2023b). 5.2.3. How to Achieve More Effective RL? We demonstrate RL works pretty well in mathematical reasoning tasks. We also provide a unified paradigm to understand different representative training methods. Within this paradigm, all methods are conceptualized as either direct or simplified RL techniques. As summarized in Equation 5, there exist three key components: Data Source, Algorithm, and Reward Function. We provide some potential future directions about the three components. Data Source Data source is the raw material of all training methods. In the context of RL, we specifically refer to the data source as the unlabeled questions with the outputs sampled from the policy model. In this paper, we only use the questions from the instruction tuning stage and a naive nucleus sampling to sample outputs. We think this is a potential reason that our RL pipeline only improves the Maj@K performance. In the future, we will explore our RL pipeline on out-of-distribution question prompts, in conjunction withadvanced sampling (decoding) strategies, like those based on",
             "value": "Top K rather than the enhancement of fundamental capabilities. Similarly, (Wang et al., 2023a) identified a misalignment problemin reasoning tasks within the SFT model, showing that the reasoning performance of SFT models can be improved through a series of preference alignment strategies (Song et al., 2023; Wang et al., 2023a; Yuan et al., 2023b). 5.2.3. How to Achieve More Effective RL? We demonstrate RL works pretty well in mathematical reasoning tasks. We also provide a unified paradigm to understand different representative training methods. Within this paradigm, all methods are conceptualized as either direct or simplified RL techniques. As summarized in Equation 5, there exist three key components: Data Source, Algorithm, and Reward Function. We provide some potential future directions about the three components. Data Source Data source is the raw material of all training methods. In the context of RL, we specifically refer to the data source as the unlabeled questions with the outputs sampled from the policy model. In this paper, we only use the questions from the instruction tuning stage and a naive nucleus sampling to sample outputs. We think this is a potential reason that our RL pipeline only improves the Maj@K performance. In the future, we will explore our RL pipeline on out-of-distribution question prompts, in conjunction withadvanced sampling (decoding) strategies, like those based on",
             "unit": "",
             "context": "Top K rather than the enhancement of fundamental capabilities. Similarly, (Wang et al., 2023a) identified a misalignment problemin reasoning tasks within the SFT model, showing that the reasoning performance of SFT models can be improved through a series of preference alignment strategies (Song et al., 2023; Wang et al., 2023a; Yuan et al., 2023b). 5.2.3. How to Achieve More Effective RL? We demonstrate RL works pretty well in mathematical reasoning tasks. We also provide a unified paradigm to understand different representative training methods. Within this paradigm, all methods are conceptualized as either direct or simplified RL techniques. As summarized in Equation 5, there exist three key components: Data Source, Algorithm, and Reward Function. We provide some potential future directions about the three components. Data Source Data source is the raw material of all training methods. In the context of RL, we specifically refer to the data source as the unlabeled questions with the outputs sampled from the policy model. In this paper, we only use the questions from the instruction tuning stage and a naive nucleus sampling to sample outputs. We think this is a potential reason that our RL pipeline only improves the Maj@K performance. In the future, we will explore our RL pipeline on out-of-distribution question prompts, in conjunction withadvanced sampling (decoding) strategies, like those based on",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658663251_vpl5u967b_1754658860766_42_gjzrxq",
             "sourceDocument": "GRPO_Papper.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/(?:best|top|highest|superior|outperforms?|better than|exceeds)[^\\n]*/gi",
               "patternDescription": "Ranking indicators",
               "fullMatch": "Top K rather than the enhancement of fundamental capabilities. Similarly, (Wang et al., 2023a) identified a misalignment problemin reasoning tasks within the SFT model, showing that the reasoning performance of SFT models can be improved through a series of preference alignment strategies (Song et al., 2023; Wang et al., 2023a; Yuan et al., 2023b). 5.2.3. How to Achieve More Effective RL? We demonstrate RL works pretty well in mathematical reasoning tasks. We also provide a unified paradigm to understand different representative training methods. Within this paradigm, all methods are conceptualized as either direct or simplified RL techniques. As summarized in Equation 5, there exist three key components: Data Source, Algorithm, and Reward Function. We provide some potential future directions about the three components. Data Source Data source is the raw material of all training methods. In the context of RL, we specifically refer to the data source as the unlabeled questions with the outputs sampled from the policy model. In this paper, we only use the questions from the instruction tuning stage and a naive nucleus sampling to sample outputs. We think this is a potential reason that our RL pipeline only improves the Maj@K performance. In the future, we will explore our RL pipeline on out-of-distribution question prompts, in conjunction withadvanced sampling (decoding) strategies, like those based on"
             }
           },
           {
             "content": "outperforms both So TA foundation models and human experts. This research demonstrates that RL-trained LLMs can serve as effective tools for complex engineering optimization, potentially transforming engineering domains beyond software development. 1 Introduction Large Language Models (LLMs) have significantly transformed software engineering practices, yielding quantifiable improvements in code generation, debugging processes, and documentation development. Studies demonstrate productivity enhancements of 55% on various software development task completion Git[1]. Despite these documented efficiency gains in computational domains, LLMs’ applications to mechanical, aerospace, civil, and other physical engineering disciplines remain underdeveloped. This disparity raises a fundamental research question: Can LLMs function as effective tools for engineering tasks beyond software development? To explore this hypothesis, we selected high-powered rocketry as an ideal test domain for several compelling reasons. First, rocket design represents a well-bounded yet complex engineering challenge",
             "value": "outperforms both So TA foundation models and human experts. This research demonstrates that RL-trained LLMs can serve as effective tools for complex engineering optimization, potentially transforming engineering domains beyond software development. 1 Introduction Large Language Models (LLMs) have significantly transformed software engineering practices, yielding quantifiable improvements in code generation, debugging processes, and documentation development. Studies demonstrate productivity enhancements of 55% on various software development task completion Git[1]. Despite these documented efficiency gains in computational domains, LLMs’ applications to mechanical, aerospace, civil, and other physical engineering disciplines remain underdeveloped. This disparity raises a fundamental research question: Can LLMs function as effective tools for engineering tasks beyond software development? To explore this hypothesis, we selected high-powered rocketry as an ideal test domain for several compelling reasons. First, rocket design represents a well-bounded yet complex engineering challenge",
             "unit": "",
             "context": "outperforms both So TA foundation models and human experts. This research demonstrates that RL-trained LLMs can serve as effective tools for complex engineering optimization, potentially transforming engineering domains beyond software development. 1 Introduction Large Language Models (LLMs) have significantly transformed software engineering practices, yielding quantifiable improvements in code generation, debugging processes, and documentation development. Studies demonstrate productivity enhancements of 55% on various software development task completion Git[1]. Despite these documented efficiency gains in computational domains, LLMs’ applications to mechanical, aerospace, civil, and other physical engineering disciplines remain underdeveloped. This disparity raises a fundamental research question: Can LLMs function as effective tools for engineering tasks beyond software development? To explore this hypothesis, we selected high-powered rocketry as an ideal test domain for several compelling reasons. First, rocket design represents a well-bounded yet complex engineering challenge",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658915966_7rvdl7exw_1754658932262_0_6anhpg",
             "sourceDocument": "LLMS FOR ENGINEERING- TEACHING MODELS TO DESIGN HIGH POWERED ROCKETS_2504.19394v2.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/(?:best|top|highest|superior|outperforms?|better than|exceeds)[^\\n]*/gi",
               "patternDescription": "Ranking indicators",
               "fullMatch": "outperforms both So TA foundation models and human experts. This research demonstrates that RL-trained LLMs can serve as effective tools for complex engineering optimization, potentially transforming engineering domains beyond software development. 1 Introduction Large Language Models (LLMs) have significantly transformed software engineering practices, yielding quantifiable improvements in code generation, debugging processes, and documentation development. Studies demonstrate productivity enhancements of 55% on various software development task completion Git[1]. Despite these documented efficiency gains in computational domains, LLMs’ applications to mechanical, aerospace, civil, and other physical engineering disciplines remain underdeveloped. This disparity raises a fundamental research question: Can LLMs function as effective tools for engineering tasks beyond software development? To explore this hypothesis, we selected high-powered rocketry as an ideal test domain for several compelling reasons. First, rocket design represents a well-bounded yet complex engineering challenge"
             }
           },
           {
             "content": "superior computational efficiency (Fawzi et al., 2022). Similarly, Alpha Dev[7] optimized low-level sorting algorithms beyond human-designed solutions, and Alpha Chip[9] enhanced semiconductor design through systematic exploration of parameter spaces. These systems primarily leverage traditional RL methodologies with domain-specific reward structures. Our approach diverges by employing language models as the base policy, leveraging their baseline engineering knowledge and physical reasoning capabilities. This integration of structured domain priors with reinforcement learning enables more sample-efficient optimization while maintaining domain flexibility, addressing a fundamental limitation of previous approaches that require extensive task-specific engineering. 3 Methodology 3.1 Simulation Env Our simulations were built on Rocket Py, a high-fidelity trajectory simulation library for high-power rocketry. Rocket Py provides a complete 6 degrees of freedom (6-DOF) simulation framework that accounts for variable mass effects, aerodynamic forces, and parachute descent phases with exceptional accuracy. The library has been validated against real-world flight data, demonstrating relative errors of less than 2% for apogee predictions across multiple documented test flights[2]. For our research, we enhanced Rocket Py with custom design rule checks (DRCs) and timeout mechanisms to address common failure modes observed during initial testing. DRCs included basic constraints including verification that body diameter exceeded motor diameter and that body length was greater than motor length to ensure proper component integration. These additions prevent simulation failures caused by physically impossible configurations and terminate excessively long computations that often result from unrealistic rocket parameters. To enable LLMs to interface with the simulation environment, we developed a",
             "value": "superior computational efficiency (Fawzi et al., 2022). Similarly, Alpha Dev[7] optimized low-level sorting algorithms beyond human-designed solutions, and Alpha Chip[9] enhanced semiconductor design through systematic exploration of parameter spaces. These systems primarily leverage traditional RL methodologies with domain-specific reward structures. Our approach diverges by employing language models as the base policy, leveraging their baseline engineering knowledge and physical reasoning capabilities. This integration of structured domain priors with reinforcement learning enables more sample-efficient optimization while maintaining domain flexibility, addressing a fundamental limitation of previous approaches that require extensive task-specific engineering. 3 Methodology 3.1 Simulation Env Our simulations were built on Rocket Py, a high-fidelity trajectory simulation library for high-power rocketry. Rocket Py provides a complete 6 degrees of freedom (6-DOF) simulation framework that accounts for variable mass effects, aerodynamic forces, and parachute descent phases with exceptional accuracy. The library has been validated against real-world flight data, demonstrating relative errors of less than 2% for apogee predictions across multiple documented test flights[2]. For our research, we enhanced Rocket Py with custom design rule checks (DRCs) and timeout mechanisms to address common failure modes observed during initial testing. DRCs included basic constraints including verification that body diameter exceeded motor diameter and that body length was greater than motor length to ensure proper component integration. These additions prevent simulation failures caused by physically impossible configurations and terminate excessively long computations that often result from unrealistic rocket parameters. To enable LLMs to interface with the simulation environment, we developed a",
             "unit": "",
             "context": "superior computational efficiency (Fawzi et al., 2022). Similarly, Alpha Dev[7] optimized low-level sorting algorithms beyond human-designed solutions, and Alpha Chip[9] enhanced semiconductor design through systematic exploration of parameter spaces. These systems primarily leverage traditional RL methodologies with domain-specific reward structures. Our approach diverges by employing language models as the base policy, leveraging their baseline engineering knowledge and physical reasoning capabilities. This integration of structured domain priors with reinforcement learning enables more sample-efficient optimization while maintaining domain flexibility, addressing a fundamental limitation of previous approaches that require extensive task-specific engineering. 3 Methodology 3.1 Simulation Env Our simulations were built on Rocket Py, a high-fidelity trajectory simulation library for high-power rocketry. Rocket Py provides a complete 6 degrees of freedom (6-DOF) simulation framework that accounts for variable mass effects, aerodynamic forces, and parachute descent phases with exceptional accuracy. The library has been validated against real-world flight data, demonstrating relative errors of less than 2% for apogee predictions across multiple documented test flights[2]. For our research, we enhanced Rocket Py with custom design rule checks (DRCs) and timeout mechanisms to address common failure modes observed during initial testing. DRCs included basic constraints including verification that body diameter exceeded motor diameter and that body length was greater than motor length to ensure proper component integration. These additions prevent simulation failures caused by physically impossible configurations and terminate excessively long computations that often result from unrealistic rocket parameters. To enable LLMs to interface with the simulation environment, we developed a",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658915966_7rvdl7exw_1754658932262_3_r4zwex",
             "sourceDocument": "LLMS FOR ENGINEERING- TEACHING MODELS TO DESIGN HIGH POWERED ROCKETS_2504.19394v2.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/(?:best|top|highest|superior|outperforms?|better than|exceeds)[^\\n]*/gi",
               "patternDescription": "Ranking indicators",
               "fullMatch": "superior computational efficiency (Fawzi et al., 2022). Similarly, Alpha Dev[7] optimized low-level sorting algorithms beyond human-designed solutions, and Alpha Chip[9] enhanced semiconductor design through systematic exploration of parameter spaces. These systems primarily leverage traditional RL methodologies with domain-specific reward structures. Our approach diverges by employing language models as the base policy, leveraging their baseline engineering knowledge and physical reasoning capabilities. This integration of structured domain priors with reinforcement learning enables more sample-efficient optimization while maintaining domain flexibility, addressing a fundamental limitation of previous approaches that require extensive task-specific engineering. 3 Methodology 3.1 Simulation Env Our simulations were built on Rocket Py, a high-fidelity trajectory simulation library for high-power rocketry. Rocket Py provides a complete 6 degrees of freedom (6-DOF) simulation framework that accounts for variable mass effects, aerodynamic forces, and parachute descent phases with exceptional accuracy. The library has been validated against real-world flight data, demonstrating relative errors of less than 2% for apogee predictions across multiple documented test flights[2]. For our research, we enhanced Rocket Py with custom design rule checks (DRCs) and timeout mechanisms to address common failure modes observed during initial testing. DRCs included basic constraints including verification that body diameter exceeded motor diameter and that body length was greater than motor length to ensure proper component integration. These additions prevent simulation failures caused by physically impossible configurations and terminate excessively long computations that often result from unrealistic rocket parameters. To enable LLMs to interface with the simulation environment, we developed a"
             }
           },
           {
             "content": "Top Radius Upper radius of tail section in meters Bottom Radius Lower radius of tail section in meters Material Selection from available materials Parachutes Main CD×S Main parachute drag area Main Trigger Deployment trigger condition Drogue CD×S Drogue parachute drag area Drogue Trigger Deployment trigger condition Launch Rail Length Launch rail length in meters Inclination Launch angle from horizontal in degrees Heading Compass heading in degrees Payload Mass Payload mass in kilograms Position Relative position from rocket center in meters 3.2 Competition Tasks We developed two increasingly complex tasks to evaluate model performance across a spectrum of engineering challenges. While the weighting and selection criteria for each scoring component were chosen heuristically, they were designed to approximate the evaluation frameworks used in actual rocket competitions. 3.2.1 Target Altitude Challenge The first task was inspired by the 10,000-foot category of the Spaceport America Cup, an international collegiate rocket engineering competition. In this event, teams must design rockets that reach as close as possible to the target altitude while ensuring safe recovery and operational integrity. Our reward function was designed to capture the essential performance metrics of this competition, focusing on flight performance aspects that could be simulated. The reward balanced multiple objectives with the following components: •Altitude Accuracy (50%): A linear reward based on the percentage difference between the achieved apogee and the target altitude:",
             "value": "Top Radius Upper radius of tail section in meters Bottom Radius Lower radius of tail section in meters Material Selection from available materials Parachutes Main CD×S Main parachute drag area Main Trigger Deployment trigger condition Drogue CD×S Drogue parachute drag area Drogue Trigger Deployment trigger condition Launch Rail Length Launch rail length in meters Inclination Launch angle from horizontal in degrees Heading Compass heading in degrees Payload Mass Payload mass in kilograms Position Relative position from rocket center in meters 3.2 Competition Tasks We developed two increasingly complex tasks to evaluate model performance across a spectrum of engineering challenges. While the weighting and selection criteria for each scoring component were chosen heuristically, they were designed to approximate the evaluation frameworks used in actual rocket competitions. 3.2.1 Target Altitude Challenge The first task was inspired by the 10,000-foot category of the Spaceport America Cup, an international collegiate rocket engineering competition. In this event, teams must design rockets that reach as close as possible to the target altitude while ensuring safe recovery and operational integrity. Our reward function was designed to capture the essential performance metrics of this competition, focusing on flight performance aspects that could be simulated. The reward balanced multiple objectives with the following components: •Altitude Accuracy (50%): A linear reward based on the percentage difference between the achieved apogee and the target altitude:",
             "unit": "",
             "context": "Top Radius Upper radius of tail section in meters Bottom Radius Lower radius of tail section in meters Material Selection from available materials Parachutes Main CD×S Main parachute drag area Main Trigger Deployment trigger condition Drogue CD×S Drogue parachute drag area Drogue Trigger Deployment trigger condition Launch Rail Length Launch rail length in meters Inclination Launch angle from horizontal in degrees Heading Compass heading in degrees Payload Mass Payload mass in kilograms Position Relative position from rocket center in meters 3.2 Competition Tasks We developed two increasingly complex tasks to evaluate model performance across a spectrum of engineering challenges. While the weighting and selection criteria for each scoring component were chosen heuristically, they were designed to approximate the evaluation frameworks used in actual rocket competitions. 3.2.1 Target Altitude Challenge The first task was inspired by the 10,000-foot category of the Spaceport America Cup, an international collegiate rocket engineering competition. In this event, teams must design rockets that reach as close as possible to the target altitude while ensuring safe recovery and operational integrity. Our reward function was designed to capture the essential performance metrics of this competition, focusing on flight performance aspects that could be simulated. The reward balanced multiple objectives with the following components: •Altitude Accuracy (50%): A linear reward based on the percentage difference between the achieved apogee and the target altitude:",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658915966_7rvdl7exw_1754658932262_6_mpfcno",
             "sourceDocument": "LLMS FOR ENGINEERING- TEACHING MODELS TO DESIGN HIGH POWERED ROCKETS_2504.19394v2.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/(?:best|top|highest|superior|outperforms?|better than|exceeds)[^\\n]*/gi",
               "patternDescription": "Ranking indicators",
               "fullMatch": "Top Radius Upper radius of tail section in meters Bottom Radius Lower radius of tail section in meters Material Selection from available materials Parachutes Main CD×S Main parachute drag area Main Trigger Deployment trigger condition Drogue CD×S Drogue parachute drag area Drogue Trigger Deployment trigger condition Launch Rail Length Launch rail length in meters Inclination Launch angle from horizontal in degrees Heading Compass heading in degrees Payload Mass Payload mass in kilograms Position Relative position from rocket center in meters 3.2 Competition Tasks We developed two increasingly complex tasks to evaluate model performance across a spectrum of engineering challenges. While the weighting and selection criteria for each scoring component were chosen heuristically, they were designed to approximate the evaluation frameworks used in actual rocket competitions. 3.2.1 Target Altitude Challenge The first task was inspired by the 10,000-foot category of the Spaceport America Cup, an international collegiate rocket engineering competition. In this event, teams must design rockets that reach as close as possible to the target altitude while ensuring safe recovery and operational integrity. Our reward function was designed to capture the essential performance metrics of this competition, focusing on flight performance aspects that could be simulated. The reward balanced multiple objectives with the following components: •Altitude Accuracy (50%): A linear reward based on the percentage difference between the achieved apogee and the target altitude:"
             }
           },
           {
             "content": "Best over 3 runs for LLMs) 15101520253020 30 40 50 60 70 80 Iteration Performance Score Model Performance Over 30 Iterations o1Claude Best Human Performance Figure 2: Performance comparison of Claude vs. o1 over 30 iterations with best human performance indicated. superior ability to iterate on designs based on simulation feedback. Despite strong baseline capabilities and consistent improvement trends, all LLMs ultimately fell short of human performance in iterative design optimization. We next investigated whether additional iterations would enable models to surpass human performance. Our 30-iteration experiment with Claude 3.7 and o1 (Figure 2 yielded particularly insightful results about the models’ improvement limitations. Both models plateaued below the human maximum despite extensive iteration opportunities. Claude 3.7 stabilized around 75.73, coming close to but never surpassing the human record of 76.57, while o1 plateaued at a lower 67.7. This persistent gap indicates that neither model could match the human level performance despite having triple the iteration cycles compared to the baseline experiment. We also evaluated \"best-of-10\" sampling alongside our primary methods, observing significant performance gains across models. The improvements varied by model: GPT-4o showed the largest relative improvement, gaining 12.89 points to reach 56.51; o1 increased from 60.57 to 68.3; Claude 3.7 rose from 62.14 to 69.69; and Deepseek v3 improved from 57.3 to 63.28. Despite these gains,",
             "value": "Best over 3 runs for LLMs) 15101520253020 30 40 50 60 70 80 Iteration Performance Score Model Performance Over 30 Iterations o1Claude Best Human Performance Figure 2: Performance comparison of Claude vs. o1 over 30 iterations with best human performance indicated. superior ability to iterate on designs based on simulation feedback. Despite strong baseline capabilities and consistent improvement trends, all LLMs ultimately fell short of human performance in iterative design optimization. We next investigated whether additional iterations would enable models to surpass human performance. Our 30-iteration experiment with Claude 3.7 and o1 (Figure 2 yielded particularly insightful results about the models’ improvement limitations. Both models plateaued below the human maximum despite extensive iteration opportunities. Claude 3.7 stabilized around 75.73, coming close to but never surpassing the human record of 76.57, while o1 plateaued at a lower 67.7. This persistent gap indicates that neither model could match the human level performance despite having triple the iteration cycles compared to the baseline experiment. We also evaluated \"best-of-10\" sampling alongside our primary methods, observing significant performance gains across models. The improvements varied by model: GPT-4o showed the largest relative improvement, gaining 12.89 points to reach 56.51; o1 increased from 60.57 to 68.3; Claude 3.7 rose from 62.14 to 69.69; and Deepseek v3 improved from 57.3 to 63.28. Despite these gains,",
             "unit": "",
             "context": "Best over 3 runs for LLMs) 15101520253020 30 40 50 60 70 80 Iteration Performance Score Model Performance Over 30 Iterations o1Claude Best Human Performance Figure 2: Performance comparison of Claude vs. o1 over 30 iterations with best human performance indicated. superior ability to iterate on designs based on simulation feedback. Despite strong baseline capabilities and consistent improvement trends, all LLMs ultimately fell short of human performance in iterative design optimization. We next investigated whether additional iterations would enable models to surpass human performance. Our 30-iteration experiment with Claude 3.7 and o1 (Figure 2 yielded particularly insightful results about the models’ improvement limitations. Both models plateaued below the human maximum despite extensive iteration opportunities. Claude 3.7 stabilized around 75.73, coming close to but never surpassing the human record of 76.57, while o1 plateaued at a lower 67.7. This persistent gap indicates that neither model could match the human level performance despite having triple the iteration cycles compared to the baseline experiment. We also evaluated \"best-of-10\" sampling alongside our primary methods, observing significant performance gains across models. The improvements varied by model: GPT-4o showed the largest relative improvement, gaining 12.89 points to reach 56.51; o1 increased from 60.57 to 68.3; Claude 3.7 rose from 62.14 to 69.69; and Deepseek v3 improved from 57.3 to 63.28. Despite these gains,",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658915966_7rvdl7exw_1754658932262_12_bbe4jv",
             "sourceDocument": "LLMS FOR ENGINEERING- TEACHING MODELS TO DESIGN HIGH POWERED ROCKETS_2504.19394v2.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/(?:best|top|highest|superior|outperforms?|better than|exceeds)[^\\n]*/gi",
               "patternDescription": "Ranking indicators",
               "fullMatch": "Best over 3 runs for LLMs) 15101520253020 30 40 50 60 70 80 Iteration Performance Score Model Performance Over 30 Iterations o1Claude Best Human Performance Figure 2: Performance comparison of Claude vs. o1 over 30 iterations with best human performance indicated. superior ability to iterate on designs based on simulation feedback. Despite strong baseline capabilities and consistent improvement trends, all LLMs ultimately fell short of human performance in iterative design optimization. We next investigated whether additional iterations would enable models to surpass human performance. Our 30-iteration experiment with Claude 3.7 and o1 (Figure 2 yielded particularly insightful results about the models’ improvement limitations. Both models plateaued below the human maximum despite extensive iteration opportunities. Claude 3.7 stabilized around 75.73, coming close to but never surpassing the human record of 76.57, while o1 plateaued at a lower 67.7. This persistent gap indicates that neither model could match the human level performance despite having triple the iteration cycles compared to the baseline experiment. We also evaluated \"best-of-10\" sampling alongside our primary methods, observing significant performance gains across models. The improvements varied by model: GPT-4o showed the largest relative improvement, gaining 12.89 points to reach 56.51; o1 increased from 60.57 to 68.3; Claude 3.7 rose from 62.14 to 69.69; and Deepseek v3 improved from 57.3 to 63.28. Despite these gains,"
             }
           },
           {
             "content": "best models scoring 19.82 points below human experts. This pattern suggests that while current LLMs have internalized substantial engineering principles from their training, they lack the strategic iteration abilities that human experts employ when refining designs. These findings indicate that while LLMs show promise as engineering tools for generating initial designs and baseline solutions, they currently cannot match human experts’ ability to iteratively refine complex engineering systems through feedback-driven optimization. 7 ----------------Page (6) Break---------------- ar Xiv Template A PREPRINT 01020304050 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Training Step Reward Target Altitude Challenge 010203040506070800 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Training Step Reward Precision Landing Challenge Figure 4: Comparison of RL performance. Reward represents mean batch reward 4.2 Reinforcement Learning",
             "value": "best models scoring 19.82 points below human experts. This pattern suggests that while current LLMs have internalized substantial engineering principles from their training, they lack the strategic iteration abilities that human experts employ when refining designs. These findings indicate that while LLMs show promise as engineering tools for generating initial designs and baseline solutions, they currently cannot match human experts’ ability to iteratively refine complex engineering systems through feedback-driven optimization. 7 ----------------Page (6) Break---------------- ar Xiv Template A PREPRINT 01020304050 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Training Step Reward Target Altitude Challenge 010203040506070800 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Training Step Reward Precision Landing Challenge Figure 4: Comparison of RL performance. Reward represents mean batch reward 4.2 Reinforcement Learning",
             "unit": "",
             "context": "best models scoring 19.82 points below human experts. This pattern suggests that while current LLMs have internalized substantial engineering principles from their training, they lack the strategic iteration abilities that human experts employ when refining designs. These findings indicate that while LLMs show promise as engineering tools for generating initial designs and baseline solutions, they currently cannot match human experts’ ability to iteratively refine complex engineering systems through feedback-driven optimization. 7 ----------------Page (6) Break---------------- ar Xiv Template A PREPRINT 01020304050 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Training Step Reward Target Altitude Challenge 010203040506070800 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Training Step Reward Precision Landing Challenge Figure 4: Comparison of RL performance. Reward represents mean batch reward 4.2 Reinforcement Learning",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658915966_7rvdl7exw_1754658932262_15_246btk",
             "sourceDocument": "LLMS FOR ENGINEERING- TEACHING MODELS TO DESIGN HIGH POWERED ROCKETS_2504.19394v2.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/(?:best|top|highest|superior|outperforms?|better than|exceeds)[^\\n]*/gi",
               "patternDescription": "Ranking indicators",
               "fullMatch": "best models scoring 19.82 points below human experts. This pattern suggests that while current LLMs have internalized substantial engineering principles from their training, they lack the strategic iteration abilities that human experts employ when refining designs. These findings indicate that while LLMs show promise as engineering tools for generating initial designs and baseline solutions, they currently cannot match human experts’ ability to iteratively refine complex engineering systems through feedback-driven optimization. 7 ----------------Page (6) Break---------------- ar Xiv Template A PREPRINT 01020304050 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Training Step Reward Target Altitude Challenge 010203040506070800 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Training Step Reward Precision Landing Challenge Figure 4: Comparison of RL performance. Reward represents mean batch reward 4.2 Reinforcement Learning"
             }
           },
           {
             "content": "Best Rocket Design from RL on Precision Landing Challenge on the models. While this provides a fair comparison, we acknowledge that human experts with additional iterations or collaborative environments might develop superior solutions. Despite these limitations, our results demonstrate that RL-enhanced LLMs can outperform individual human experts under controlled conditions, establishing a meaningful benchmark for future research. 6 Discussion 6.1 RL vs Inference Time Compute Our results indicate that inference time compute scaling works across both simple and complex engineering tasks but current methods still plateau below human level performance. We observe that reasoning-focused models (o1) outperform foundation models (GPT-4o), suggesting that further scaling of the reasoning model paradigm could yield additional performance improvements. Test-time reinforcement learning (TTRL) represents another promising axis for scaling compute to enhance performance. Beyond providing an incremental boost in specific domains, TTRL offers a methodology for extending performance boundaries in individual domains, potentially previewing future capabilities of large language models. 6.2 Why use LLMs for RL? Despite impressive successes like Alpha Go and Alpha Zero",
             "value": "Best Rocket Design from RL on Precision Landing Challenge on the models. While this provides a fair comparison, we acknowledge that human experts with additional iterations or collaborative environments might develop superior solutions. Despite these limitations, our results demonstrate that RL-enhanced LLMs can outperform individual human experts under controlled conditions, establishing a meaningful benchmark for future research. 6 Discussion 6.1 RL vs Inference Time Compute Our results indicate that inference time compute scaling works across both simple and complex engineering tasks but current methods still plateau below human level performance. We observe that reasoning-focused models (o1) outperform foundation models (GPT-4o), suggesting that further scaling of the reasoning model paradigm could yield additional performance improvements. Test-time reinforcement learning (TTRL) represents another promising axis for scaling compute to enhance performance. Beyond providing an incremental boost in specific domains, TTRL offers a methodology for extending performance boundaries in individual domains, potentially previewing future capabilities of large language models. 6.2 Why use LLMs for RL? Despite impressive successes like Alpha Go and Alpha Zero",
             "unit": "",
             "context": "Best Rocket Design from RL on Precision Landing Challenge on the models. While this provides a fair comparison, we acknowledge that human experts with additional iterations or collaborative environments might develop superior solutions. Despite these limitations, our results demonstrate that RL-enhanced LLMs can outperform individual human experts under controlled conditions, establishing a meaningful benchmark for future research. 6 Discussion 6.1 RL vs Inference Time Compute Our results indicate that inference time compute scaling works across both simple and complex engineering tasks but current methods still plateau below human level performance. We observe that reasoning-focused models (o1) outperform foundation models (GPT-4o), suggesting that further scaling of the reasoning model paradigm could yield additional performance improvements. Test-time reinforcement learning (TTRL) represents another promising axis for scaling compute to enhance performance. Beyond providing an incremental boost in specific domains, TTRL offers a methodology for extending performance boundaries in individual domains, potentially previewing future capabilities of large language models. 6.2 Why use LLMs for RL? Despite impressive successes like Alpha Go and Alpha Zero",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658915966_7rvdl7exw_1754658932262_18_njs105",
             "sourceDocument": "LLMS FOR ENGINEERING- TEACHING MODELS TO DESIGN HIGH POWERED ROCKETS_2504.19394v2.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/(?:best|top|highest|superior|outperforms?|better than|exceeds)[^\\n]*/gi",
               "patternDescription": "Ranking indicators",
               "fullMatch": "Best Rocket Design from RL on Precision Landing Challenge on the models. While this provides a fair comparison, we acknowledge that human experts with additional iterations or collaborative environments might develop superior solutions. Despite these limitations, our results demonstrate that RL-enhanced LLMs can outperform individual human experts under controlled conditions, establishing a meaningful benchmark for future research. 6 Discussion 6.1 RL vs Inference Time Compute Our results indicate that inference time compute scaling works across both simple and complex engineering tasks but current methods still plateau below human level performance. We observe that reasoning-focused models (o1) outperform foundation models (GPT-4o), suggesting that further scaling of the reasoning model paradigm could yield additional performance improvements. Test-time reinforcement learning (TTRL) represents another promising axis for scaling compute to enhance performance. Beyond providing an incremental boost in specific domains, TTRL offers a methodology for extending performance boundaries in individual domains, potentially previewing future capabilities of large language models. 6.2 Why use LLMs for RL? Despite impressive successes like Alpha Go and Alpha Zero"
             }
           },
           {
             "content": "best design 14 ----------------Page (13) Break----------------",
             "value": "best design 14 ----------------Page (13) Break----------------",
             "unit": "",
             "context": "best design 14 ----------------Page (13) Break----------------",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658915966_7rvdl7exw_1754658932262_28_yr2y0b",
             "sourceDocument": "LLMS FOR ENGINEERING- TEACHING MODELS TO DESIGN HIGH POWERED ROCKETS_2504.19394v2.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/(?:best|top|highest|superior|outperforms?|better than|exceeds)[^\\n]*/gi",
               "patternDescription": "Ranking indicators",
               "fullMatch": "best design 14 ----------------Page (13) Break----------------"
             }
           },
           {
             "content": ".",
             "value": ".",
             "unit": "",
             "context": "against real-world flight data, demonstrating relative errors of less than 2% for apogee predictions across multiple documented test flights[2]. For our research, we enhanced Rocket Py with custom design rule checks (DRCs) and timeout mechanisms to address common failure modes observed during initial testing. DRCs included basic constraints including verification that body diameter exceeded motor diameter and that body length was greater than motor length to ensure proper component integration. These additions prevent simulation failures caused by physically impossible configurations and terminate excessively long computations that often result from unrealistic rocket parameters. To enable LLMs to interface with the simulation environment, we developed a",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658915966_7rvdl7exw_1754658932262_3_r4zwex",
             "sourceDocument": "LLMS FOR ENGINEERING- TEACHING MODELS TO DESIGN HIGH POWERED ROCKETS_2504.19394v2.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/(?:vs|versus|compared to|against)[^\\n]*([\\d.]+%?)[^\\n]*/gi",
               "patternDescription": "Comparative metrics",
               "fullMatch": "against real-world flight data, demonstrating relative errors of less than 2% for apogee predictions across multiple documented test flights[2]. For our research, we enhanced Rocket Py with custom design rule checks (DRCs) and timeout mechanisms to address common failure modes observed during initial testing. DRCs included basic constraints including verification that body diameter exceeded motor diameter and that body length was greater than motor length to ensure proper component integration. These additions prevent simulation failures caused by physically impossible configurations and terminate excessively long computations that often result from unrealistic rocket parameters. To enable LLMs to interface with the simulation environment, we developed a"
             }
           },
           {
             "content": "2",
             "value": "2",
             "unit": "",
             "context": "vs Inference Time Compute Our results indicate that inference time compute scaling works across both simple and complex engineering tasks but current methods still plateau below human level performance. We observe that reasoning-focused models (o1) outperform foundation models (GPT-4o), suggesting that further scaling of the reasoning model paradigm could yield additional performance improvements. Test-time reinforcement learning (TTRL) represents another promising axis for scaling compute to enhance performance. Beyond providing an incremental boost in specific domains, TTRL offers a methodology for extending performance boundaries in individual domains, potentially previewing future capabilities of large language models. 6.2 Why use LLMs for RL? Despite impressive successes like Alpha Go and Alpha Zero",
             "confidence": 0.95,
             "sourceChunkId": "chunk_doc_1754658915966_7rvdl7exw_1754658932262_18_njs105",
             "sourceDocument": "LLMS FOR ENGINEERING- TEACHING MODELS TO DESIGN HIGH POWERED ROCKETS_2504.19394v2.pdf",
             "metadata": {
               "extractionMethod": "regex_pattern",
               "regexPattern": "/(?:vs|versus|compared to|against)[^\\n]*([\\d.]+%?)[^\\n]*/gi",
               "patternDescription": "Comparative metrics",
               "fullMatch": "vs Inference Time Compute Our results indicate that inference time compute scaling works across both simple and complex engineering tasks but current methods still plateau below human level performance. We observe that reasoning-focused models (o1) outperform foundation models (GPT-4o), suggesting that further scaling of the reasoning model paradigm could yield additional performance improvements. Test-time reinforcement learning (TTRL) represents another promising axis for scaling compute to enhance performance. Beyond providing an incremental boost in specific domains, TTRL offers a methodology for extending performance boundaries in individual domains, potentially previewing future capabilities of large language models. 6.2 Why use LLMs for RL? Despite impressive successes like Alpha Go and Alpha Zero"
             }
           }
         ],
         "structured": []
       },
       "itemCount": 32,
       "reasoning": "Data extraction completed"
     }
   }

───────────────────────────────────

Total Agents: 4
Completed: 4
Generated: 09/08/2025, 01:12:37
